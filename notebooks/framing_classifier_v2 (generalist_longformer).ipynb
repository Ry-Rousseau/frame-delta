{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e493299e",
   "metadata": {},
   "source": [
    "### Framing classifier using BERT - V2 - Longformer Generalist\n",
    "\n",
    "Retain all features from V1 notebook. Drop the long-doc policy in favor of the longformer. Adding in title + text into input. Introduce a more streamlined means of storing results, including folders per run. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3539415b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import transformers\n",
    "load_dotenv()  # looks for .env in current directory or parent\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76920f34",
   "metadata": {},
   "source": [
    "### Sample the data, NO Topic Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f5fa3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Column(name='text_generic_frame', type_code=1009), Column(name='gpt_topic', type_code=25), Column(name='political_leaning', type_code=25), Column(name='title', type_code=25), Column(name='maintext', type_code=25))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_generic_frame</th>\n",
       "      <th>gpt_topic</th>\n",
       "      <th>political_leaning</th>\n",
       "      <th>title</th>\n",
       "      <th>article_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Cultural identity, External regulation and re...</td>\n",
       "      <td>War &amp; Conflict</td>\n",
       "      <td>left_lean</td>\n",
       "      <td>Suspects arrested after Moscow concert hall at...</td>\n",
       "      <td>State media reported Saturday that Russian aut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Legality, constitutionality and jurisprudence...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>right</td>\n",
       "      <td>Senate prepares for Mayorkas impeachment artic...</td>\n",
       "      <td>NEWYou can now listen to Fox News articles!\\nT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Economic, Political]</td>\n",
       "      <td>Politics</td>\n",
       "      <td>left_lean</td>\n",
       "      <td>Tim Scott's presidential campaign is burning t...</td>\n",
       "      <td>Few 2024 GOP presidential hopefuls are bringin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Cultural identity, Fairness and equality, Ext...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>left_lean</td>\n",
       "      <td>USA v Portugal: Women’s World Cup 2023 Group E...</td>\n",
       "      <td>Key events\\nShow key events only\\nPlease turn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Policy prescription and evaluation, Crime and...</td>\n",
       "      <td>Legal</td>\n",
       "      <td>right</td>\n",
       "      <td>Alex Murdaugh 'extremely angry' about jury tam...</td>\n",
       "      <td>NEWYou can now listen to Fox News articles!\\nA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  text_generic_frame       gpt_topic  \\\n",
       "0  [Cultural identity, External regulation and re...  War & Conflict   \n",
       "1  [Legality, constitutionality and jurisprudence...        Politics   \n",
       "2                              [Economic, Political]        Politics   \n",
       "3  [Cultural identity, Fairness and equality, Ext...          Sports   \n",
       "4  [Policy prescription and evaluation, Crime and...           Legal   \n",
       "\n",
       "  political_leaning                                              title  \\\n",
       "0         left_lean  Suspects arrested after Moscow concert hall at...   \n",
       "1             right  Senate prepares for Mayorkas impeachment artic...   \n",
       "2         left_lean  Tim Scott's presidential campaign is burning t...   \n",
       "3         left_lean  USA v Portugal: Women’s World Cup 2023 Group E...   \n",
       "4             right  Alex Murdaugh 'extremely angry' about jury tam...   \n",
       "\n",
       "                                        article_text  \n",
       "0  State media reported Saturday that Russian aut...  \n",
       "1  NEWYou can now listen to Fox News articles!\\nT...  \n",
       "2  Few 2024 GOP presidential hopefuls are bringin...  \n",
       "3  Key events\\nShow key events only\\nPlease turn ...  \n",
       "4  NEWYou can now listen to Fox News articles!\\nA...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to server \n",
    "import psycopg2\n",
    "conn = psycopg2.connect(\n",
    "    dbname=os.getenv(\"DB_NAME\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    port=os.getenv(\"DB_PORT\")\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# key: set the seed\n",
    "cur.execute(\"SELECT setseed(0.42)\")\n",
    "\n",
    "# Do our join in database - NOTE this is with a POLITICS FILTER\n",
    "cur.execute(f\"\"\"\n",
    "           SELECT a.text_generic_frame, a.gpt_topic, a.political_leaning, a.title,  \n",
    "           b.maintext\n",
    "           FROM mm_framing_full a\n",
    "           JOIN newsarticles b ON a.url = b.url\n",
    "           ORDER BY RANDOM()\n",
    "            LIMIT 75000\n",
    "            \"\"\")\n",
    "\n",
    "result= cur.fetchall()\n",
    "\n",
    "print(cur.description)\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "df = pd.DataFrame(result, columns=[\"text_generic_frame\", \"gpt_topic\", \"political_leaning\", \"title\", \"article_text\"])\n",
    "\n",
    "del result\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e8039",
   "metadata": {},
   "source": [
    "### Initial Data Filtering\n",
    "\n",
    "Not that in between these notebooks, I changed the gpt_topic and generic frame columns directly in database so I don't have to keep fixing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a34bf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 75000\n",
      "Filtered rows: 56406\n"
     ]
    }
   ],
   "source": [
    "# Create word count column\n",
    "df['num_words'] = df['article_text'].str.split().str.len()\n",
    "\n",
    "print(f\"Original rows: {len(df)}\")\n",
    "\n",
    "# Filter based on length\n",
    "df_filtered = df[(df['num_words'] > 100)]\n",
    "df = df_filtered.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(f\"Filtered rows: {len(df_filtered)}\")\n",
    "\n",
    "del df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10160ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows remaining: 55579\n"
     ]
    }
   ],
   "source": [
    "# Keep rows only where the list is NOT exactly ['Other']\n",
    "df = df[df['text_generic_frame'].apply(lambda x: x != ['Other'])]\n",
    "print(f\"Rows remaining: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfe6b9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_generic_frame</th>\n",
       "      <th>gpt_topic</th>\n",
       "      <th>political_leaning</th>\n",
       "      <th>title</th>\n",
       "      <th>article_text</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Cultural identity, External regulation and re...</td>\n",
       "      <td>War &amp; Conflict</td>\n",
       "      <td>left_lean</td>\n",
       "      <td>Suspects arrested after Moscow concert hall at...</td>\n",
       "      <td>State media reported Saturday that Russian aut...</td>\n",
       "      <td>887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Legality, constitutionality and jurisprudence...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>right</td>\n",
       "      <td>Senate prepares for Mayorkas impeachment artic...</td>\n",
       "      <td>NEWYou can now listen to Fox News articles!\\nT...</td>\n",
       "      <td>602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Cultural identity, Fairness and equality, Ext...</td>\n",
       "      <td>Sports</td>\n",
       "      <td>left_lean</td>\n",
       "      <td>USA v Portugal: Women’s World Cup 2023 Group E...</td>\n",
       "      <td>Key events\\nShow key events only\\nPlease turn ...</td>\n",
       "      <td>1527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Policy prescription and evaluation, Crime and...</td>\n",
       "      <td>Legal</td>\n",
       "      <td>right</td>\n",
       "      <td>Alex Murdaugh 'extremely angry' about jury tam...</td>\n",
       "      <td>NEWYou can now listen to Fox News articles!\\nA...</td>\n",
       "      <td>936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Fairness and equality, Political, Public opin...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>left_lean</td>\n",
       "      <td>Nikki Haley dropping out of GOP presidential race</td>\n",
       "      <td>Former United Nations Ambassador Nikki Haley s...</td>\n",
       "      <td>1003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  text_generic_frame       gpt_topic  \\\n",
       "0  [Cultural identity, External regulation and re...  War & Conflict   \n",
       "1  [Legality, constitutionality and jurisprudence...        Politics   \n",
       "2  [Cultural identity, Fairness and equality, Ext...          Sports   \n",
       "3  [Policy prescription and evaluation, Crime and...           Legal   \n",
       "4  [Fairness and equality, Political, Public opin...        Politics   \n",
       "\n",
       "  political_leaning                                              title  \\\n",
       "0         left_lean  Suspects arrested after Moscow concert hall at...   \n",
       "1             right  Senate prepares for Mayorkas impeachment artic...   \n",
       "2         left_lean  USA v Portugal: Women’s World Cup 2023 Group E...   \n",
       "3             right  Alex Murdaugh 'extremely angry' about jury tam...   \n",
       "4         left_lean  Nikki Haley dropping out of GOP presidential race   \n",
       "\n",
       "                                        article_text  num_words  \n",
       "0  State media reported Saturday that Russian aut...        887  \n",
       "1  NEWYou can now listen to Fox News articles!\\nT...        602  \n",
       "2  Key events\\nShow key events only\\nPlease turn ...       1527  \n",
       "3  NEWYou can now listen to Fox News articles!\\nA...        936  \n",
       "4  Former United Nations Ambassador Nikki Haley s...       1003  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26905695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer the text column\n",
    "\n",
    "# Adding the title\n",
    "df['article_text'] = df['title'] + \"\\n\" + df['article_text']\n",
    "\n",
    "# Adding the topic at the very start\n",
    "df['article_text'] = \"TOPIC:\" + df['gpt_topic'] + \"\\n\" + df['article_text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5794a4c8",
   "metadata": {},
   "source": [
    "### Tokenization + Adaptively setting max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "221a66bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring token lengths...\n",
      "Mean Length: 740.0\n",
      "95th Percentile: 1449.0 tokens\n",
      "99th Percentile: 1922.7 tokens\n",
      "Max Length found: 20301 tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import LongformerTokenizerFast\n",
    "import numpy as np\n",
    "\n",
    "# 1. Initialize Tokenizer\n",
    "model_name = \"allenai/longformer-base-4096\"\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "# 2. Measure Lengths\n",
    "# We process in batches to keep it snappy\n",
    "print(\"Measuring token lengths...\")\n",
    "token_lens = []\n",
    "texts = df['article_text'].tolist()\n",
    "\n",
    "# Tokenize just to count (no padding/truncation yet)\n",
    "# Using the fast tokenizer's batch_encode_plus is usually efficient enough\n",
    "encodings = tokenizer(texts, add_special_tokens=True, return_attention_mask=False)\n",
    "token_lens = [len(x) for x in encodings['input_ids']]\n",
    "\n",
    "# 3. Statistics\n",
    "token_lens = np.array(token_lens)\n",
    "p95 = np.percentile(token_lens, 95)\n",
    "p99 = np.percentile(token_lens, 99)\n",
    "\n",
    "print(f\"Mean Length: {np.mean(token_lens):.1f}\")\n",
    "print(f\"95th Percentile: {p95:.1f} tokens\")\n",
    "print(f\"99th Percentile: {p99:.1f} tokens\")\n",
    "print(f\"Max Length found: {np.max(token_lens)} tokens\")\n",
    "\n",
    "# based on these a max_length of 2048 is sufficient, and balances efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c297b06",
   "metadata": {},
   "source": [
    "### Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af478bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55579, 15)\n"
     ]
    }
   ],
   "source": [
    "# load the binarizer\n",
    "import joblib\n",
    "mlb = joblib.load('encoders/mlb_15_classes.pkl')\n",
    "\n",
    "labels_matrix = mlb.transform(df['text_generic_frame'])\n",
    "\n",
    "print(labels_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef908797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can innovate in storing meta data from past approach by just storing self.df\n",
    "# we also innovate by saving VRAM with the __getitem__ implementation\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NewsArticleDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, labels_matrix, max_len=2048):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        # We store the pre-computed matrix you made with MLB\n",
    "        self.labels = labels_matrix \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = str(row['article_text'])\n",
    "        \n",
    "        # TOKENIZATION\n",
    "        # Truncate, but DO NOT PAD here. \n",
    "        # The collator handles padding to save memory.\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding=False, \n",
    "            add_special_tokens=True \n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        \n",
    "        # CREATE GLOBAL ATTENTION MASK\n",
    "        # 0 = Local Attention, 1 = Global Attention\n",
    "        # We strictly set the [CLS] token (index 0) to Global Attention.\n",
    "        global_attention_mask = [0] * len(input_ids)\n",
    "        global_attention_mask[0] = 1 \n",
    "        \n",
    "        # GET LABELS\n",
    "        # Directly access the row from your matrix\n",
    "        labels_vec = self.labels[idx]\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'global_attention_mask': global_attention_mask,\n",
    "            'labels': torch.tensor(labels_vec, dtype=torch.float),\n",
    "            \n",
    "            # METADATA PASS-THROUGH\n",
    "            'metadata': {\n",
    "                'url': row.get('url', ''),\n",
    "                'title': row.get('title', ''),\n",
    "                'gpt_topic': row.get('gpt_topic', ''),\n",
    "                'num_words': row.get('num_words', 0)\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33cd6839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function, which is critical to running this much larger model\n",
    "\n",
    "def longformer_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collator to handle dynamic padding and 512-window alignment.\n",
    "    \"\"\"\n",
    "    # 1. Determine the maximum length in this specific batch\n",
    "    max_len = max(len(item['input_ids']) for item in batch)\n",
    "    \n",
    "    # 2. Round up to nearest multiple of 512 (Longformer Window Size)\n",
    "    # This aligns memory for the sliding window attention mechanism\n",
    "    window_size = 512\n",
    "    padded_len = ((max_len + window_size - 1) // window_size) * window_size\n",
    "    \n",
    "    # Prepare batch lists\n",
    "    input_ids_batch = []\n",
    "    attention_mask_batch = []\n",
    "    global_attention_mask_batch = []\n",
    "    labels_batch = []\n",
    "    metadata_batch = []\n",
    "    \n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    for item in batch:\n",
    "        # Calculate padding needed for this sequence\n",
    "        curr_len = len(item['input_ids'])\n",
    "        pad_len = padded_len - curr_len\n",
    "        \n",
    "        # Pad Input IDs\n",
    "        ids = item['input_ids'] + [pad_token_id] * pad_len\n",
    "        \n",
    "        # Pad Attention Mask (0 for padded tokens)\n",
    "        mask = item['attention_mask'] + [0] * pad_len\n",
    "        \n",
    "        # Pad Global Attention Mask (0 for padded tokens)\n",
    "        global_mask = item['global_attention_mask'] + [0] * pad_len\n",
    "        \n",
    "        input_ids_batch.append(ids)\n",
    "        attention_mask_batch.append(mask)\n",
    "        global_attention_mask_batch.append(global_mask)\n",
    "        labels_batch.append(item['labels'])\n",
    "        metadata_batch.append(item['metadata'])\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids_batch, dtype=torch.long),\n",
    "        'attention_mask': torch.tensor(attention_mask_batch, dtype=torch.long),\n",
    "        'global_attention_mask': torch.tensor(global_attention_mask_batch, dtype=torch.long),\n",
    "        'labels': torch.stack(labels_batch),\n",
    "        'metadata': metadata_batch # Returns a list of dicts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5075abbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits created:\n",
      "Train: 44463 | Val: 5558 | Test: 5558\n",
      "Loaders ready. Train batches: 11116\n"
     ]
    }
   ],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# 1. SETUP SPLITTERS -----------------------------------------------------------\n",
    "# We need indices to split.\n",
    "N = len(labels_matrix)\n",
    "X_indices = np.zeros(N) # Dummy features just to satisfy the splitter API\n",
    "\n",
    "# A. Split Train (80%) vs Temp (20%)\n",
    "msss1 = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n",
    "train_idx, temp_idx = next(iter(msss1.split(X_indices, labels_matrix)))\n",
    "\n",
    "# B. Split Temp into Val (10%) and Test (10%)\n",
    "# We split the *temp* indices in half\n",
    "temp_labels = labels_matrix[temp_idx]\n",
    "temp_dummy_X = np.zeros(len(temp_idx))\n",
    "\n",
    "msss2 = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.50, random_state=42)\n",
    "relative_val_idx, relative_test_idx = next(iter(msss2.split(temp_dummy_X, temp_labels)))\n",
    "\n",
    "# Map relative indices back to original dataframe indices\n",
    "val_idx = temp_idx[relative_val_idx]\n",
    "test_idx = temp_idx[relative_test_idx]\n",
    "\n",
    "print(f\"Splits created:\")\n",
    "print(f\"Train: {len(train_idx)} | Val: {len(val_idx)} | Test: {len(test_idx)}\")\n",
    "\n",
    "\n",
    "# 2. INSTANTIATE DATASETS ------------------------------------------------------\n",
    "# We create ONE full dataset, then subset it using the indices above.\n",
    "full_dataset = NewsArticleDataset(\n",
    "    df, \n",
    "    tokenizer, \n",
    "    labels_matrix, \n",
    "    max_len=2048\n",
    ")\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "val_dataset   = Subset(full_dataset, val_idx)\n",
    "test_dataset  = Subset(full_dataset, test_idx)\n",
    "\n",
    "\n",
    "# 3. CREATE DATALOADERS --------------------------------------------------------\n",
    "# Optimizations for RTX 4070 Ti Super\n",
    "BATCH_SIZE = 4 \n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, # Shuffle ONLY training\n",
    "    collate_fn=longformer_collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=longformer_collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=longformer_collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Loaders ready. Train batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d4510a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': '', 'title': 'Greek government under fire after video shows ‘pushback’ of asylum seekers', 'gpt_topic': 'Politics', 'num_words': np.int64(663)}\n"
     ]
    }
   ],
   "source": [
    "# Let's test on a batch as a sanity check\n",
    "# grab a batch using iterator next()\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(batch['metadata'][1])\n",
    "# so now we have our useful metadata in our batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d0635",
   "metadata": {},
   "source": [
    "### Pre-training Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a39a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a clean state, hard reset the GPU state\n",
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d609139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Longformer (this may take a moment)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and moved to GPU.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datetime import datetime\n",
    "# 1. THE DEEP IMPORT (Go straight to the source file)\n",
    "try:\n",
    "    from transformers.models.longformer.modeling_longformer import LongformerForSequenceClassification\n",
    "except ImportError:\n",
    "    # If this fails, we try the old directory structure (sometimes happens in older/conda envs)\n",
    "    from transformers.models.longformer import LongformerForSequenceClassification\n",
    "\n",
    "# 2. MODEL INITIALIZATION -----------------------------------------------------\n",
    "print(\"Loading Longformer (this may take a moment)...\")\n",
    "\n",
    "model = LongformerForSequenceClassification.from_pretrained(\n",
    "    \"allenai/longformer-base-4096\",\n",
    "    num_labels=15, \n",
    "    problem_type=\"multi_label_classification\",\n",
    "    ignore_mismatched_sizes=True \n",
    ")\n",
    "\n",
    "# ENABLE Gradient Checkpointing - which saves a huge amount of V-Ram\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "model.to('cuda')\n",
    "print(\"Model loaded and moved to GPU.\")\n",
    "\n",
    "# 3. OPTIMIZER & SCHEDULER ----------------------------------------------------\n",
    "# Hyperparameters\n",
    "LR = 3e-5\n",
    "EPOCHS = 4\n",
    "# set the number of mini batches you run before updating gradients\n",
    "ACCUMULATION_STEPS = 4  # Effective Batch Size = 4 (physical) * 4 (accum) = 16\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=LR \n",
    ")\n",
    "\n",
    "# 4. THE \"LAB MANAGER\" (EXPERIMENT TRACKER) -----------------------------------\n",
    "class ExperimentTracker:\n",
    "    def __init__(self, run_name, base_dir=\"saved_models/framing_training_runs_longformer\"):\n",
    "        # Create a unique timestamped folder for this run\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "        self.run_dir = os.path.join(base_dir, f\"{timestamp}_{run_name}\")\n",
    "        os.makedirs(self.run_dir, exist_ok=True)\n",
    "        print(f\" Experiment initialized. Saving to: {self.run_dir}\")\n",
    "        \n",
    "        # Initialize a log dictionary\n",
    "        self.history = {\n",
    "            \"config\": {\n",
    "                \"model\": \"longformer-base-4096\",\n",
    "                \"max_len\": 2048,\n",
    "                \"batch_size\": 4, \n",
    "                \"accum_steps\": ACCUMULATION_STEPS,\n",
    "                \"lr\": LR\n",
    "            },\n",
    "            \"epochs\": []\n",
    "        }\n",
    "        \n",
    "    def log_epoch(self, epoch_data):\n",
    "        \"\"\"Append epoch results to history and save immediately.\"\"\"\n",
    "        self.history[\"epochs\"].append(epoch_data)\n",
    "        self.save_history()\n",
    "        \n",
    "    def save_history(self):\n",
    "        with open(os.path.join(self.run_dir, \"metrics.json\"), \"w\") as f:\n",
    "            json.dump(self.history, f, indent=4)\n",
    "            \n",
    "    def save_model(self, model, name=\"model_state.bin\"):\n",
    "        torch.save(model.state_dict(), os.path.join(self.run_dir, name))\n",
    "        print(f\"Model saved: {name}\")\n",
    "\n",
    "    def save_report(self, df_report, name=\"classification_report.csv\"):\n",
    "        df_report.to_csv(os.path.join(self.run_dir, name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23b81899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated positive weights for 15 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# DEFINE weighted loss\n",
    "\n",
    "num_positives = torch.tensor(labels_matrix.sum(axis=0), dtype=torch.float)\n",
    "num_negatives = len(labels_matrix) - num_positives\n",
    "\n",
    "# Calculate ratio: if we have 10x more negatives, we boost positives by 10x\n",
    "pos_weight = (num_negatives / (num_positives + 1e-5)).to('cuda')\n",
    "\n",
    "print(f\"Calculated positive weights for {len(pos_weight)} classes.\")\n",
    "\n",
    "# 2. Define the Criterion\n",
    "# We pass this into the training function\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aae8ac",
   "metadata": {},
   "source": [
    "### Training Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "860fae48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Experiment initialized. Saving to: saved_models/framing_training_runs_longformer\\20260121_0143_longformer_topic_expert_v1\n"
     ]
    }
   ],
   "source": [
    "# Initialize and name the run\n",
    "tracker = ExperimentTracker(run_name=\"longformer_topic_expert_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef6ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 3. TRAINING ENGINE ----------------------------------------------------------\n",
    "def train_engine(model, train_loader, val_loader, optimizer, tracker, criterion):\n",
    "    scaler = torch.amp.GradScaler('cuda') # Mixed Precision (Newer PyTorch syntax)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n======== EPOCH {epoch+1}/{EPOCHS} ========\")\n",
    "        \n",
    "        # --- TRAINING PHASE ---\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loop = tqdm(train_loader, leave=True)\n",
    "        for step, batch in enumerate(loop):\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to('cuda')\n",
    "            attention_mask = batch['attention_mask'].to('cuda')\n",
    "            global_attention_mask = batch['global_attention_mask'].to('cuda')\n",
    "            labels = batch['labels'].to('cuda')\n",
    "            \n",
    "            # Forward Pass (Mixed Precision)\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    global_attention_mask=global_attention_mask,\n",
    "                    # We pass labels only to suppress warnings, we don't use internal loss\n",
    "                    labels=labels \n",
    "                )\n",
    "                \n",
    "                # CUSTOM WEIGHTED LOSS\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "                loss = loss / ACCUMULATION_STEPS\n",
    "            \n",
    "            # Backward Pass\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (step + 1) % ACCUMULATION_STEPS == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            train_loss += loss.item() * ACCUMULATION_STEPS\n",
    "            loop.set_postfix(loss=loss.item() * ACCUMULATION_STEPS)\n",
    "            \n",
    "        if len(train_loader) % ACCUMULATION_STEPS != 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # --- VALIDATION PHASE ---\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        print(\"Running Validation...\")\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to('cuda')\n",
    "                attention_mask = batch['attention_mask'].to('cuda')\n",
    "                global_attention_mask = batch['global_attention_mask'].to('cuda')\n",
    "                labels = batch['labels'].to('cuda')\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    global_attention_mask=global_attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                loss = criterion(outputs.logits, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                probs = torch.sigmoid(outputs.logits)\n",
    "                all_preds.append(probs.cpu().numpy())\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # --- METRICS ---\n",
    "        all_preds_np = np.concatenate(all_preds)\n",
    "        all_labels_np = np.concatenate(all_labels)\n",
    "\n",
    "        # 1. Standard Monitor (Micro @ 0.5) - Good for overall accuracy\n",
    "        temp_preds = (all_preds_np > 0.5).astype(int)\n",
    "        val_f1_micro = f1_score(all_labels_np, temp_preds, average='micro')\n",
    "\n",
    "        # 2. Minority Class Monitor (Macro @ 0.5) - Good for rare frames\n",
    "        # This warns you if the model is just predicting the dominant classes\n",
    "        val_f1_macro = f1_score(all_labels_np, temp_preds, average='macro')\n",
    "\n",
    "        print(f\" Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Micro-F1: {val_f1_micro:.4f} | Macro-F1: {val_f1_macro:.4f}\")\n",
    "\n",
    "        tracker.log_epoch({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"val_f1_micro\": val_f1_micro,\n",
    "            \"val_f1_macro\": val_f1_macro \n",
    "        })\n",
    "        \n",
    "        # Save per-epoch model\n",
    "        tracker.save_model(model, name=f\"model_ep{epoch+1}.bin\")\n",
    "\n",
    "    print(\"Training Complete.\")\n",
    "    tracker.save_model(model, name=\"final_model.bin\")\n",
    "    \n",
    "    return all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03080d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. EXECUTE TRAINING ---------------------------------------------------------\n",
    "print(\"Starting Training Run...\")\n",
    "val_preds, val_targets = train_engine(model, train_loader, val_loader, optimizer, tracker, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "framing_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
