{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e493299e",
   "metadata": {},
   "source": [
    "### Framing classifier using BERT - V2 - Longformer Generalist\n",
    "\n",
    "Retain all features from V1 notebook. Drop the long-doc policy in favor of the longformer. Adding in title + text into input. Introduce a more streamlined means of storing results, including folders per run. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3539415b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import transformers\n",
    "load_dotenv()  # looks for .env in current directory or parent\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76920f34",
   "metadata": {},
   "source": [
    "### Sample the data, NO Topic Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f5fa3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Column(name='text_generic_frame', type_code=1009), Column(name='gpt_topic', type_code=25), Column(name='political_leaning', type_code=25), Column(name='title', type_code=25), Column(name='maintext', type_code=25))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_generic_frame</th>\n",
       "      <th>gpt_topic</th>\n",
       "      <th>political_leaning</th>\n",
       "      <th>title</th>\n",
       "      <th>article_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Health and safety, Quality of life, Other]</td>\n",
       "      <td>Sports</td>\n",
       "      <td>left_lean</td>\n",
       "      <td>James Harden scores his 25,000th point, leads ...</td>\n",
       "      <td>James Harden scored his 25,000th career point ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Political]</td>\n",
       "      <td>Politics</td>\n",
       "      <td>left_lean</td>\n",
       "      <td>Look ahead to the biggest political stories of...</td>\n",
       "      <td>Political reporter Jack Fink and CBS News Texa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Capacity and resources, Policy prescription a...</td>\n",
       "      <td>Business &amp; Economy</td>\n",
       "      <td>left_lean</td>\n",
       "      <td>UK housing market fears as Marshalls and Purpl...</td>\n",
       "      <td>A profit warning from the UK driveways to roof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Political, Policy prescription and evaluation]</td>\n",
       "      <td>Politics</td>\n",
       "      <td>left_lean</td>\n",
       "      <td>Cook County State’s Attorney's race, Bring Chi...</td>\n",
       "      <td>The State's Attorney's race is still up in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Cultural identity, Fairness and equality, Mor...</td>\n",
       "      <td>Education</td>\n",
       "      <td>left</td>\n",
       "      <td>Florida Teacher Investigated After Showing Dis...</td>\n",
       "      <td>LOADINGERROR LOADING\\nA Florida elementary sch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  text_generic_frame           gpt_topic  \\\n",
       "0        [Health and safety, Quality of life, Other]              Sports   \n",
       "1                                        [Political]            Politics   \n",
       "2  [Capacity and resources, Policy prescription a...  Business & Economy   \n",
       "3    [Political, Policy prescription and evaluation]            Politics   \n",
       "4  [Cultural identity, Fairness and equality, Mor...           Education   \n",
       "\n",
       "  political_leaning                                              title  \\\n",
       "0         left_lean  James Harden scores his 25,000th point, leads ...   \n",
       "1         left_lean  Look ahead to the biggest political stories of...   \n",
       "2         left_lean  UK housing market fears as Marshalls and Purpl...   \n",
       "3         left_lean  Cook County State’s Attorney's race, Bring Chi...   \n",
       "4              left  Florida Teacher Investigated After Showing Dis...   \n",
       "\n",
       "                                        article_text  \n",
       "0  James Harden scored his 25,000th career point ...  \n",
       "1  Political reporter Jack Fink and CBS News Texa...  \n",
       "2  A profit warning from the UK driveways to roof...  \n",
       "3  The State's Attorney's race is still up in the...  \n",
       "4  LOADINGERROR LOADING\\nA Florida elementary sch...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to server \n",
    "import psycopg2\n",
    "conn = psycopg2.connect(\n",
    "    dbname=os.getenv(\"DB_NAME\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    port=os.getenv(\"DB_PORT\")\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# key: set the seed\n",
    "cur.execute(\"SELECT setseed(0.42)\")\n",
    "\n",
    "# Do our join in database - NOTE this is with a POLITICS FILTER\n",
    "cur.execute(f\"\"\"\n",
    "           SELECT a.text_generic_frame, a.gpt_topic, a.political_leaning, a.title,  \n",
    "           b.maintext\n",
    "           FROM mm_framing_full a\n",
    "           JOIN newsarticles b ON a.url = b.url\n",
    "           ORDER BY RANDOM()\n",
    "            LIMIT 75000\n",
    "            \"\"\")\n",
    "\n",
    "result= cur.fetchall()\n",
    "\n",
    "print(cur.description)\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "df = pd.DataFrame(result, columns=[\"text_generic_frame\", \"gpt_topic\", \"political_leaning\", \"title\", \"article_text\"])\n",
    "\n",
    "del result\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e8039",
   "metadata": {},
   "source": [
    "### Initial Data Filtering\n",
    "\n",
    "Not that in between these notebooks, I changed the gpt_topic and generic frame columns directly in database so I don't have to keep fixing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a34bf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 75000\n",
      "Filtered rows: 56447\n"
     ]
    }
   ],
   "source": [
    "# Create word count column\n",
    "df['num_words'] = df['article_text'].str.split().str.len()\n",
    "\n",
    "print(f\"Original rows: {len(df)}\")\n",
    "\n",
    "# Filter based on length\n",
    "df_filtered = df[(df['num_words'] > 100)]\n",
    "df = df_filtered.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(f\"Filtered rows: {len(df_filtered)}\")\n",
    "\n",
    "del df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10160ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows remaining: 55616\n"
     ]
    }
   ],
   "source": [
    "# Keep rows only where the list is NOT exactly ['Other']\n",
    "df = df[df['text_generic_frame'].apply(lambda x: x != ['Other'])]\n",
    "print(f\"Rows remaining: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfe6b9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_generic_frame</th>\n",
       "      <th>gpt_topic</th>\n",
       "      <th>political_leaning</th>\n",
       "      <th>title</th>\n",
       "      <th>article_text</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Health and safety, Quality of life, Other]</td>\n",
       "      <td>Sports</td>\n",
       "      <td>left_lean</td>\n",
       "      <td>James Harden scores his 25,000th point, leads ...</td>\n",
       "      <td>James Harden scored his 25,000th career point ...</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Capacity and resources, Policy prescription a...</td>\n",
       "      <td>Business &amp; Economy</td>\n",
       "      <td>left_lean</td>\n",
       "      <td>UK housing market fears as Marshalls and Purpl...</td>\n",
       "      <td>A profit warning from the UK driveways to roof...</td>\n",
       "      <td>536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Cultural identity, Fairness and equality, Mor...</td>\n",
       "      <td>Education</td>\n",
       "      <td>left</td>\n",
       "      <td>Florida Teacher Investigated After Showing Dis...</td>\n",
       "      <td>LOADINGERROR LOADING\\nA Florida elementary sch...</td>\n",
       "      <td>464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Crime and punishment, Public opinion, Securit...</td>\n",
       "      <td>Crime &amp; Safety</td>\n",
       "      <td>left_lean</td>\n",
       "      <td>Police: Two armed minors attempt to rob man si...</td>\n",
       "      <td>BALTIMORE - A man was sitting in his car Thurs...</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Economic, Capacity and resources, Public opin...</td>\n",
       "      <td>Social Issues</td>\n",
       "      <td>left_lean</td>\n",
       "      <td>150-year-old Florida Keys lighthouse illuminat...</td>\n",
       "      <td>ISLAMORADA, Fla. (AP) — A 150-year-old beacon ...</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  text_generic_frame           gpt_topic  \\\n",
       "0        [Health and safety, Quality of life, Other]              Sports   \n",
       "1  [Capacity and resources, Policy prescription a...  Business & Economy   \n",
       "2  [Cultural identity, Fairness and equality, Mor...           Education   \n",
       "3  [Crime and punishment, Public opinion, Securit...      Crime & Safety   \n",
       "4  [Economic, Capacity and resources, Public opin...       Social Issues   \n",
       "\n",
       "  political_leaning                                              title  \\\n",
       "0         left_lean  James Harden scores his 25,000th point, leads ...   \n",
       "1         left_lean  UK housing market fears as Marshalls and Purpl...   \n",
       "2              left  Florida Teacher Investigated After Showing Dis...   \n",
       "3         left_lean  Police: Two armed minors attempt to rob man si...   \n",
       "4         left_lean  150-year-old Florida Keys lighthouse illuminat...   \n",
       "\n",
       "                                        article_text  num_words  \n",
       "0  James Harden scored his 25,000th career point ...        747  \n",
       "1  A profit warning from the UK driveways to roof...        536  \n",
       "2  LOADINGERROR LOADING\\nA Florida elementary sch...        464  \n",
       "3  BALTIMORE - A man was sitting in his car Thurs...        258  \n",
       "4  ISLAMORADA, Fla. (AP) — A 150-year-old beacon ...        278  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26905695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer the text column\n",
    "\n",
    "# Adding the title\n",
    "df['article_text'] = df['title'] + \"\\n\" + df['article_text']\n",
    "\n",
    "# Adding the topic at the very start\n",
    "df['article_text'] = \"TOPIC:\" + df['gpt_topic'] + \"\\n\" + df['article_text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5794a4c8",
   "metadata": {},
   "source": [
    "### Tokenization + Adaptively setting max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "221a66bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring token lengths...\n",
      "Mean Length: 740.5\n",
      "95th Percentile: 1452.0 tokens\n",
      "99th Percentile: 1914.8 tokens\n",
      "Max Length found: 21541 tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import LongformerTokenizerFast\n",
    "import numpy as np\n",
    "\n",
    "# 1. Initialize Tokenizer\n",
    "model_name = \"allenai/longformer-base-4096\"\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "# 2. Measure Lengths\n",
    "# We process in batches to keep it snappy\n",
    "print(\"Measuring token lengths...\")\n",
    "token_lens = []\n",
    "texts = df['article_text'].tolist()\n",
    "\n",
    "# Tokenize just to count (no padding/truncation yet)\n",
    "# Using the fast tokenizer's batch_encode_plus is usually efficient enough\n",
    "encodings = tokenizer(texts, add_special_tokens=True, return_attention_mask=False)\n",
    "token_lens = [len(x) for x in encodings['input_ids']]\n",
    "\n",
    "# 3. Statistics\n",
    "token_lens = np.array(token_lens)\n",
    "p95 = np.percentile(token_lens, 95)\n",
    "p99 = np.percentile(token_lens, 99)\n",
    "\n",
    "print(f\"Mean Length: {np.mean(token_lens):.1f}\")\n",
    "print(f\"95th Percentile: {p95:.1f} tokens\")\n",
    "print(f\"99th Percentile: {p99:.1f} tokens\")\n",
    "print(f\"Max Length found: {np.max(token_lens)} tokens\")\n",
    "\n",
    "# based on these a max_length of 2048 is sufficient, and balances efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c297b06",
   "metadata": {},
   "source": [
    "### Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af478bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55616, 15)\n"
     ]
    }
   ],
   "source": [
    "# load the binarizer\n",
    "import joblib\n",
    "mlb = joblib.load('encoders/mlb_15_classes.pkl')\n",
    "\n",
    "labels_matrix = mlb.transform(df['text_generic_frame'])\n",
    "\n",
    "print(labels_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef908797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can innovate in storing meta data from past approach by just storing self.df\n",
    "# we also innovate by saving VRAM with the __getitem__ implementation\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NewsArticleDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, labels_matrix, max_len=2048):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        # We store the pre-computed matrix you made with MLB\n",
    "        self.labels = labels_matrix \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = str(row['article_text'])\n",
    "        \n",
    "        # TOKENIZATION\n",
    "        # Truncate, but DO NOT PAD here. \n",
    "        # The collator handles padding to save memory.\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding=False, \n",
    "            add_special_tokens=True \n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        \n",
    "        # CREATE GLOBAL ATTENTION MASK\n",
    "        # 0 = Local Attention, 1 = Global Attention\n",
    "        # We strictly set the [CLS] token (index 0) to Global Attention.\n",
    "        global_attention_mask = [0] * len(input_ids)\n",
    "        global_attention_mask[0] = 1 \n",
    "        \n",
    "        # GET LABELS\n",
    "        # Directly access the row from your matrix\n",
    "        labels_vec = self.labels[idx]\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'global_attention_mask': global_attention_mask,\n",
    "            'labels': torch.tensor(labels_vec, dtype=torch.float),\n",
    "            \n",
    "            # METADATA PASS-THROUGH\n",
    "            'metadata': {\n",
    "                'url': row.get('url', ''),\n",
    "                'title': row.get('title', ''),\n",
    "                'gpt_topic': row.get('gpt_topic', ''),\n",
    "                'num_words': row.get('num_words', 0)\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33cd6839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function, which is critical to running this much larger model\n",
    "\n",
    "def longformer_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collator to handle dynamic padding and 512-window alignment.\n",
    "    \"\"\"\n",
    "    # 1. Determine the maximum length in this specific batch\n",
    "    max_len = max(len(item['input_ids']) for item in batch)\n",
    "    \n",
    "    # 2. Round up to nearest multiple of 512 (Longformer Window Size)\n",
    "    # This aligns memory for the sliding window attention mechanism\n",
    "    window_size = 512\n",
    "    padded_len = ((max_len + window_size - 1) // window_size) * window_size\n",
    "    \n",
    "    # Prepare batch lists\n",
    "    input_ids_batch = []\n",
    "    attention_mask_batch = []\n",
    "    global_attention_mask_batch = []\n",
    "    labels_batch = []\n",
    "    metadata_batch = []\n",
    "    \n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    for item in batch:\n",
    "        # Calculate padding needed for this sequence\n",
    "        curr_len = len(item['input_ids'])\n",
    "        pad_len = padded_len - curr_len\n",
    "        \n",
    "        # Pad Input IDs\n",
    "        ids = item['input_ids'] + [pad_token_id] * pad_len\n",
    "        \n",
    "        # Pad Attention Mask (0 for padded tokens)\n",
    "        mask = item['attention_mask'] + [0] * pad_len\n",
    "        \n",
    "        # Pad Global Attention Mask (0 for padded tokens)\n",
    "        global_mask = item['global_attention_mask'] + [0] * pad_len\n",
    "        \n",
    "        input_ids_batch.append(ids)\n",
    "        attention_mask_batch.append(mask)\n",
    "        global_attention_mask_batch.append(global_mask)\n",
    "        labels_batch.append(item['labels'])\n",
    "        metadata_batch.append(item['metadata'])\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids_batch, dtype=torch.long),\n",
    "        'attention_mask': torch.tensor(attention_mask_batch, dtype=torch.long),\n",
    "        'global_attention_mask': torch.tensor(global_attention_mask_batch, dtype=torch.long),\n",
    "        'labels': torch.stack(labels_batch),\n",
    "        'metadata': metadata_batch # Returns a list of dicts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5075abbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits created:\n",
      "Train: 44492 | Val: 5539 | Test: 5585\n",
      "Loaders ready. Train batches: 11123\n"
     ]
    }
   ],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# 1. SETUP SPLITTERS -----------------------------------------------------------\n",
    "# We need indices to split.\n",
    "N = len(labels_matrix)\n",
    "X_indices = np.zeros(N) # Dummy features just to satisfy the splitter API\n",
    "\n",
    "# A. Split Train (80%) vs Temp (20%)\n",
    "msss1 = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n",
    "train_idx, temp_idx = next(iter(msss1.split(X_indices, labels_matrix)))\n",
    "\n",
    "# B. Split Temp into Val (10%) and Test (10%)\n",
    "# We split the *temp* indices in half\n",
    "temp_labels = labels_matrix[temp_idx]\n",
    "temp_dummy_X = np.zeros(len(temp_idx))\n",
    "\n",
    "msss2 = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.50, random_state=42)\n",
    "relative_val_idx, relative_test_idx = next(iter(msss2.split(temp_dummy_X, temp_labels)))\n",
    "\n",
    "# Map relative indices back to original dataframe indices\n",
    "val_idx = temp_idx[relative_val_idx]\n",
    "test_idx = temp_idx[relative_test_idx]\n",
    "\n",
    "print(f\"Splits created:\")\n",
    "print(f\"Train: {len(train_idx)} | Val: {len(val_idx)} | Test: {len(test_idx)}\")\n",
    "\n",
    "\n",
    "# 2. INSTANTIATE DATASETS ------------------------------------------------------\n",
    "# We create ONE full dataset, then subset it using the indices above.\n",
    "full_dataset = NewsArticleDataset(\n",
    "    df, \n",
    "    tokenizer, \n",
    "    labels_matrix, \n",
    "    max_len=2048\n",
    ")\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "val_dataset   = Subset(full_dataset, val_idx)\n",
    "test_dataset  = Subset(full_dataset, test_idx)\n",
    "\n",
    "\n",
    "# 3. CREATE DATALOADERS --------------------------------------------------------\n",
    "# Optimizations for RTX 4070 Ti Super\n",
    "BATCH_SIZE = 4 \n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, # Shuffle ONLY training\n",
    "    collate_fn=longformer_collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=longformer_collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=longformer_collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Loaders ready. Train batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d4510a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': '', 'title': \"'Real Housewives' star Brandi Glanville hospitalized following collapse, son called 911\", 'gpt_topic': 'Health', 'num_words': np.int64(351)}\n"
     ]
    }
   ],
   "source": [
    "# Let's test on a batch as a sanity check\n",
    "# grab a batch using iterator next()\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(batch['metadata'][1])\n",
    "# so now we have our useful metadata in our batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d0635",
   "metadata": {},
   "source": [
    "### Pre-training Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a39a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a clean state, hard reset the GPU state\n",
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d609139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Longformer (this may take a moment)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and moved to GPU.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datetime import datetime\n",
    "# 1. THE DEEP IMPORT (Go straight to the source file)\n",
    "try:\n",
    "    from transformers.models.longformer.modeling_longformer import LongformerForSequenceClassification\n",
    "except ImportError:\n",
    "    # If this fails, we try the old directory structure (sometimes happens in older/conda envs)\n",
    "    from transformers.models.longformer import LongformerForSequenceClassification\n",
    "\n",
    "# 2. MODEL INITIALIZATION -----------------------------------------------------\n",
    "print(\"Loading Longformer (this may take a moment)...\")\n",
    "\n",
    "model = LongformerForSequenceClassification.from_pretrained(\n",
    "    \"allenai/longformer-base-4096\",\n",
    "    num_labels=15, \n",
    "    problem_type=\"multi_label_classification\",\n",
    "    ignore_mismatched_sizes=True \n",
    ")\n",
    "\n",
    "# ENABLE Gradient Checkpointing - which saves a huge amount of V-Ram\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "model.to('cuda')\n",
    "print(\"Model loaded and moved to GPU.\")\n",
    "\n",
    "# 3. OPTIMIZER & SCHEDULER ----------------------------------------------------\n",
    "# Hyperparameters\n",
    "LR = 3e-5\n",
    "EPOCHS = 4\n",
    "# set the number of mini batches you run before updating gradients\n",
    "ACCUMULATION_STEPS = 4  # Effective Batch Size = 4 (physical) * 4 (accum) = 16\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=LR \n",
    ")\n",
    "\n",
    "# 4. THE \"LAB MANAGER\" (EXPERIMENT TRACKER) -----------------------------------\n",
    "class ExperimentTracker:\n",
    "    def __init__(self, run_name, base_dir=\"saved_models/framing_training_runs_longformer\"):\n",
    "        # Create a unique timestamped folder for this run\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "        self.run_dir = os.path.join(base_dir, f\"{timestamp}_{run_name}\")\n",
    "        os.makedirs(self.run_dir, exist_ok=True)\n",
    "        print(f\" Experiment initialized. Saving to: {self.run_dir}\")\n",
    "        \n",
    "        # Initialize a log dictionary\n",
    "        self.history = {\n",
    "            \"config\": {\n",
    "                \"model\": \"longformer-base-4096\",\n",
    "                \"max_len\": 2048,\n",
    "                \"batch_size\": 4, \n",
    "                \"accum_steps\": ACCUMULATION_STEPS,\n",
    "                \"lr\": LR\n",
    "            },\n",
    "            \"epochs\": []\n",
    "        }\n",
    "        \n",
    "    def log_epoch(self, epoch_data):\n",
    "        \"\"\"Append epoch results to history and save immediately.\"\"\"\n",
    "        self.history[\"epochs\"].append(epoch_data)\n",
    "        self.save_history()\n",
    "        \n",
    "    def save_history(self):\n",
    "        with open(os.path.join(self.run_dir, \"metrics.json\"), \"w\") as f:\n",
    "            json.dump(self.history, f, indent=4)\n",
    "            \n",
    "    def save_model(self, model, name=\"model_state.bin\"):\n",
    "        torch.save(model.state_dict(), os.path.join(self.run_dir, name))\n",
    "        print(f\"Model saved: {name}\")\n",
    "\n",
    "    def save_report(self, df_report, name=\"classification_report.csv\"):\n",
    "        df_report.to_csv(os.path.join(self.run_dir, name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23b81899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated positive weights for 15 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# DEFINE weighted loss\n",
    "\n",
    "num_positives = torch.tensor(labels_matrix.sum(axis=0), dtype=torch.float)\n",
    "num_negatives = len(labels_matrix) - num_positives\n",
    "\n",
    "# Calculate ratio: if we have 10x more negatives, we boost positives by 10x\n",
    "pos_weight = (num_negatives / (num_positives + 1e-5)).to('cuda')\n",
    "\n",
    "print(f\"Calculated positive weights for {len(pos_weight)} classes.\")\n",
    "\n",
    "# 2. Define the Criterion\n",
    "# We pass this into the training function\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aae8ac",
   "metadata": {},
   "source": [
    "### Training Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "860fae48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Experiment initialized. Saving to: saved_models/framing_training_runs_longformer\\20260121_0143_longformer_topic_expert_v1\n"
     ]
    }
   ],
   "source": [
    "# Initialize and name the run\n",
    "tracker = ExperimentTracker(run_name=\"longformer_topic_expert_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cef6ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 3. TRAINING ENGINE ----------------------------------------------------------\n",
    "def train_engine(model, train_loader, val_loader, optimizer, tracker, criterion):\n",
    "    scaler = torch.amp.GradScaler('cuda') # Mixed Precision (Newer PyTorch syntax)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n======== EPOCH {epoch+1}/{EPOCHS} ========\")\n",
    "        \n",
    "        # --- TRAINING PHASE ---\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loop = tqdm(train_loader, leave=True)\n",
    "        for step, batch in enumerate(loop):\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to('cuda')\n",
    "            attention_mask = batch['attention_mask'].to('cuda')\n",
    "            global_attention_mask = batch['global_attention_mask'].to('cuda')\n",
    "            labels = batch['labels'].to('cuda')\n",
    "            \n",
    "            # Forward Pass (Mixed Precision)\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    global_attention_mask=global_attention_mask,\n",
    "                    # We pass labels only to suppress warnings, we don't use internal loss\n",
    "                    labels=labels \n",
    "                )\n",
    "                \n",
    "                # CUSTOM WEIGHTED LOSS\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "                loss = loss / ACCUMULATION_STEPS\n",
    "            \n",
    "            # Backward Pass\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (step + 1) % ACCUMULATION_STEPS == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            train_loss += loss.item() * ACCUMULATION_STEPS\n",
    "            loop.set_postfix(loss=loss.item() * ACCUMULATION_STEPS)\n",
    "            \n",
    "        if len(train_loader) % ACCUMULATION_STEPS != 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # --- VALIDATION PHASE ---\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        print(\"Running Validation...\")\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to('cuda')\n",
    "                attention_mask = batch['attention_mask'].to('cuda')\n",
    "                global_attention_mask = batch['global_attention_mask'].to('cuda')\n",
    "                labels = batch['labels'].to('cuda')\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    global_attention_mask=global_attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                loss = criterion(outputs.logits, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                probs = torch.sigmoid(outputs.logits)\n",
    "                all_preds.append(probs.cpu().numpy())\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # --- METRICS ---\n",
    "        all_preds_np = np.concatenate(all_preds)\n",
    "        all_labels_np = np.concatenate(all_labels)\n",
    "\n",
    "        # 1. Standard Monitor (Micro @ 0.5) - Good for overall accuracy\n",
    "        temp_preds = (all_preds_np > 0.5).astype(int)\n",
    "        val_f1_micro = f1_score(all_labels_np, temp_preds, average='micro')\n",
    "\n",
    "        # 2. Minority Class Monitor (Macro @ 0.5) - Good for rare frames\n",
    "        # This warns you if the model is just predicting the dominant classes\n",
    "        val_f1_macro = f1_score(all_labels_np, temp_preds, average='macro')\n",
    "\n",
    "        print(f\" Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Micro-F1: {val_f1_micro:.4f} | Macro-F1: {val_f1_macro:.4f}\")\n",
    "\n",
    "        tracker.log_epoch({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"val_f1_micro\": val_f1_micro,\n",
    "            \"val_f1_macro\": val_f1_macro \n",
    "        })\n",
    "        \n",
    "        # Save per-epoch model\n",
    "        tracker.save_model(model, name=f\"model_ep{epoch+1}.bin\")\n",
    "\n",
    "    print(\"Training Complete.\")\n",
    "    tracker.save_model(model, name=\"final_model.bin\")\n",
    "    \n",
    "    return all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03080d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Run...\n",
      "\n",
      "======== EPOCH 1/4 ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55947f2b25cb4bfda0b864bc7b44b2ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Validation...\n",
      " Train Loss: 0.6245 | Val Loss: 0.5768 | Micro-F1: 0.7085 | Macro-F1: 0.6850\n",
      "Model saved: model_ep1.bin\n",
      "\n",
      "======== EPOCH 2/4 ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ad59079a1c4632899bc5859949fb12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Validation...\n",
      " Train Loss: 0.5407 | Val Loss: 0.5848 | Micro-F1: 0.7188 | Macro-F1: 0.6944\n",
      "Model saved: model_ep2.bin\n",
      "\n",
      "======== EPOCH 3/4 ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc311cbeeec4fd89b19635e3c9a7c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Validation...\n",
      " Train Loss: 0.4918 | Val Loss: 0.5670 | Micro-F1: 0.7292 | Macro-F1: 0.7071\n",
      "Model saved: model_ep3.bin\n",
      "\n",
      "======== EPOCH 4/4 ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1fb188745c9463fa3e759fb480d80d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Validation...\n",
      " Train Loss: 0.4492 | Val Loss: 0.5862 | Micro-F1: 0.7270 | Macro-F1: 0.7063\n",
      "Model saved: model_ep4.bin\n",
      "Training Complete.\n",
      "Model saved: final_model.bin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. EXECUTE TRAINING ---------------------------------------------------------\n",
    "print(\"Starting Training Run...\")\n",
    "val_preds, val_targets = train_engine(model, train_loader, val_loader, optimizer, tracker, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k77ogtewgy",
   "metadata": {},
   "source": [
    "### Post-Training Evaluation: Load Best Epoch and Optimize Thresholds\n",
    "\n",
    "Based on metrics.json, Epoch 3 achieved the best validation performance:\n",
    "- Micro F1: 0.7292\n",
    "- Macro F1: 0.7071\n",
    "\n",
    "We now load this checkpoint and run per-class threshold optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tt9v6b78m6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model from: saved_models/framing_training_runs_longformer/20260121_0143_longformer_topic_expert_v1/model_ep3.bin\n"
     ]
    }
   ],
   "source": [
    "# Load best epoch model (Epoch 3)\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Path to the best model checkpoint\n",
    "best_model_path = \"saved_models/framing_training_runs_longformer/20260121_0143_longformer_topic_expert_v1/model_ep3.bin\"\n",
    "\n",
    "# Load weights into the model\n",
    "model.load_state_dict(torch.load(best_model_path, map_location='cuda'))\n",
    "model.eval()\n",
    "print(f\"Loaded best model from: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7j7h8eolgfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting validation set probabilities...\n",
      "Validation set shape: (5539, 15)\n"
     ]
    }
   ],
   "source": [
    "# Define official labels for reporting\n",
    "official_labels = [\n",
    "    \"Economic\", \"Capacity and resources\", \"Morality\", \"Fairness and equality\",\n",
    "    \"Legality, constitutionality and jurisprudence\", \"Policy prescription and evaluation\",\n",
    "    \"Crime and punishment\", \"Security and defense\", \"Health and safety\",\n",
    "    \"Quality of life\", \"Cultural identity\", \"Public opinion\", \"Political\",\n",
    "    \"External regulation and reputation\", \"Other\"\n",
    "]\n",
    "\n",
    "# Get raw probabilities (sigmoids) from the validation set for threshold optimization\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "val_probs = []\n",
    "val_labels = []\n",
    "\n",
    "print(\"Collecting validation set probabilities...\")\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to('cuda')\n",
    "        attention_mask = batch['attention_mask'].to('cuda')\n",
    "        global_attention_mask = batch['global_attention_mask'].to('cuda')\n",
    "        labels = batch['labels'].to('cuda')\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            global_attention_mask=global_attention_mask\n",
    "        )\n",
    "        probs = torch.sigmoid(outputs.logits)\n",
    "        \n",
    "        val_probs.append(probs.cpu().numpy())\n",
    "        val_labels.append(labels.cpu().numpy())\n",
    "\n",
    "val_probs = np.vstack(val_probs)\n",
    "val_labels = np.vstack(val_labels)\n",
    "\n",
    "print(f\"Validation set shape: {val_probs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f04czi2ys3l",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding optimal thresholds per class...\n",
      "  Economic                                      Best: 0.35 (Val F1: 0.803)\n",
      "  Capacity and resources                        Best: 0.85 (Val F1: 0.619)\n",
      "  Morality                                      Best: 0.85 (Val F1: 0.678)\n",
      "  Fairness and equality                         Best: 0.55 (Val F1: 0.714)\n",
      "  Legality, constitutionality and jurisprudence Best: 0.40 (Val F1: 0.801)\n",
      "  Policy prescription and evaluation            Best: 0.40 (Val F1: 0.765)\n",
      "  Crime and punishment                          Best: 0.45 (Val F1: 0.799)\n",
      "  Security and defense                          Best: 0.60 (Val F1: 0.761)\n",
      "  Health and safety                             Best: 0.60 (Val F1: 0.800)\n",
      "  Quality of life                               Best: 0.40 (Val F1: 0.807)\n",
      "  Cultural identity                             Best: 0.70 (Val F1: 0.747)\n",
      "  Public opinion                                Best: 0.45 (Val F1: 0.676)\n",
      "  Political                                     Best: 0.50 (Val F1: 0.770)\n",
      "  External regulation and reputation            Best: 0.70 (Val F1: 0.648)\n",
      "  Other                                         Best: 0.70 (Val F1: 0.522)\n"
     ]
    }
   ],
   "source": [
    "# Find the optimal threshold for EACH class via grid search\n",
    "# We test thresholds from 0.1 to 0.9 and pick the one that maximizes F1 for that specific class\n",
    "best_thresholds = np.array([0.5] * 15)  # Start with default\n",
    "n_classes = 15\n",
    "\n",
    "print(\"\\nFinding optimal thresholds per class...\")\n",
    "for i in range(n_classes):\n",
    "    best_score = 0\n",
    "    best_thresh = 0.5\n",
    "    \n",
    "    # Get just the column for this class\n",
    "    y_true = val_labels[:, i]\n",
    "    y_score = val_probs[:, i]\n",
    "    \n",
    "    # Grid search thresholds\n",
    "    for thresh in np.arange(0.1, 0.95, 0.05):  # test over increments of 0.05\n",
    "        y_pred = (y_score > thresh).astype(int)\n",
    "        score = f1_score(y_true, y_pred)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_thresh = thresh\n",
    "            \n",
    "    best_thresholds[i] = best_thresh\n",
    "    class_name = official_labels[i]\n",
    "    print(f\"  {class_name:<45} Best: {best_thresh:.2f} (Val F1: {best_score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7n81xu9e488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting test set probabilities...\n",
      "Test set shape: (5585, 15)\n"
     ]
    }
   ],
   "source": [
    "# Apply thresholds to TEST set\n",
    "print(\"\\nCollecting test set probabilities...\")\n",
    "test_probs = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to('cuda')\n",
    "        attention_mask = batch['attention_mask'].to('cuda')\n",
    "        global_attention_mask = batch['global_attention_mask'].to('cuda')\n",
    "        labels = batch['labels'].to('cuda')\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            global_attention_mask=global_attention_mask\n",
    "        )\n",
    "        probs = torch.sigmoid(outputs.logits)\n",
    "        \n",
    "        test_probs.append(probs.cpu().numpy())\n",
    "        test_labels.append(labels.cpu().numpy())\n",
    "\n",
    "test_probs = np.vstack(test_probs)\n",
    "test_labels = np.vstack(test_labels)\n",
    "\n",
    "print(f\"Test set shape: {test_probs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yi3k98lpjn",
   "metadata": {},
   "source": [
    "### Test Set Evaluation: Default Threshold (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8rdt2xtb4rr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Set Performance (Default Threshold = 0.5) ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crime and punishment</td>\n",
       "      <td>0.805340</td>\n",
       "      <td>0.820175</td>\n",
       "      <td>0.812690</td>\n",
       "      <td>2280.0</td>\n",
       "      <td>2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quality of life</td>\n",
       "      <td>0.804201</td>\n",
       "      <td>0.816998</td>\n",
       "      <td>0.810549</td>\n",
       "      <td>2765.0</td>\n",
       "      <td>2765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Legality, constitutionality and jurisprudence</td>\n",
       "      <td>0.822264</td>\n",
       "      <td>0.793321</td>\n",
       "      <td>0.807533</td>\n",
       "      <td>2216.0</td>\n",
       "      <td>2216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Economic</td>\n",
       "      <td>0.821265</td>\n",
       "      <td>0.770753</td>\n",
       "      <td>0.795207</td>\n",
       "      <td>2325.0</td>\n",
       "      <td>2325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Health and safety</td>\n",
       "      <td>0.726567</td>\n",
       "      <td>0.822348</td>\n",
       "      <td>0.771496</td>\n",
       "      <td>1593.0</td>\n",
       "      <td>1593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Political</td>\n",
       "      <td>0.738046</td>\n",
       "      <td>0.807920</td>\n",
       "      <td>0.771404</td>\n",
       "      <td>2197.0</td>\n",
       "      <td>2197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Security and defense</td>\n",
       "      <td>0.691226</td>\n",
       "      <td>0.851108</td>\n",
       "      <td>0.762880</td>\n",
       "      <td>1444.0</td>\n",
       "      <td>1444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Policy prescription and evaluation</td>\n",
       "      <td>0.761341</td>\n",
       "      <td>0.763449</td>\n",
       "      <td>0.762394</td>\n",
       "      <td>2528.0</td>\n",
       "      <td>2528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.713084</td>\n",
       "      <td>0.801541</td>\n",
       "      <td>0.749230</td>\n",
       "      <td>25960.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>micro avg</td>\n",
       "      <td>0.690516</td>\n",
       "      <td>0.801541</td>\n",
       "      <td>0.741898</td>\n",
       "      <td>25960.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>samples avg</td>\n",
       "      <td>0.717783</td>\n",
       "      <td>0.813111</td>\n",
       "      <td>0.739813</td>\n",
       "      <td>25960.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Fairness and equality</td>\n",
       "      <td>0.682447</td>\n",
       "      <td>0.781840</td>\n",
       "      <td>0.728770</td>\n",
       "      <td>1641.0</td>\n",
       "      <td>1641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>0.667706</td>\n",
       "      <td>0.805940</td>\n",
       "      <td>0.721840</td>\n",
       "      <td>25960.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Cultural identity</td>\n",
       "      <td>0.626133</td>\n",
       "      <td>0.846005</td>\n",
       "      <td>0.719649</td>\n",
       "      <td>1552.0</td>\n",
       "      <td>1552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Public opinion</td>\n",
       "      <td>0.620061</td>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.690501</td>\n",
       "      <td>2095.0</td>\n",
       "      <td>2095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>External regulation and reputation</td>\n",
       "      <td>0.579014</td>\n",
       "      <td>0.766527</td>\n",
       "      <td>0.659705</td>\n",
       "      <td>1195.0</td>\n",
       "      <td>1195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Morality</td>\n",
       "      <td>0.497768</td>\n",
       "      <td>0.874510</td>\n",
       "      <td>0.634424</td>\n",
       "      <td>510.0</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Capacity and resources</td>\n",
       "      <td>0.446451</td>\n",
       "      <td>0.842538</td>\n",
       "      <td>0.583639</td>\n",
       "      <td>851.0</td>\n",
       "      <td>851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Other</td>\n",
       "      <td>0.393465</td>\n",
       "      <td>0.752604</td>\n",
       "      <td>0.516764</td>\n",
       "      <td>768.0</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            label  precision    recall  \\\n",
       "0                            Crime and punishment   0.805340  0.820175   \n",
       "1                                 Quality of life   0.804201  0.816998   \n",
       "2   Legality, constitutionality and jurisprudence   0.822264  0.793321   \n",
       "3                                        Economic   0.821265  0.770753   \n",
       "4                               Health and safety   0.726567  0.822348   \n",
       "5                                       Political   0.738046  0.807920   \n",
       "6                            Security and defense   0.691226  0.851108   \n",
       "7              Policy prescription and evaluation   0.761341  0.763449   \n",
       "8                                    weighted avg   0.713084  0.801541   \n",
       "9                                       micro avg   0.690516  0.801541   \n",
       "10                                    samples avg   0.717783  0.813111   \n",
       "11                          Fairness and equality   0.682447  0.781840   \n",
       "12                                      macro avg   0.667706  0.805940   \n",
       "13                              Cultural identity   0.626133  0.846005   \n",
       "14                                 Public opinion   0.620061  0.778998   \n",
       "15             External regulation and reputation   0.579014  0.766527   \n",
       "16                                       Morality   0.497768  0.874510   \n",
       "17                         Capacity and resources   0.446451  0.842538   \n",
       "18                                          Other   0.393465  0.752604   \n",
       "\n",
       "    f1-score  support  count  \n",
       "0   0.812690   2280.0   2280  \n",
       "1   0.810549   2765.0   2765  \n",
       "2   0.807533   2216.0   2216  \n",
       "3   0.795207   2325.0   2325  \n",
       "4   0.771496   1593.0   1593  \n",
       "5   0.771404   2197.0   2197  \n",
       "6   0.762880   1444.0   1444  \n",
       "7   0.762394   2528.0   2528  \n",
       "8   0.749230  25960.0   <NA>  \n",
       "9   0.741898  25960.0   <NA>  \n",
       "10  0.739813  25960.0   <NA>  \n",
       "11  0.728770   1641.0   1641  \n",
       "12  0.721840  25960.0   <NA>  \n",
       "13  0.719649   1552.0   1552  \n",
       "14  0.690501   2095.0   2095  \n",
       "15  0.659705   1195.0   1195  \n",
       "16  0.634424    510.0    510  \n",
       "17  0.583639    851.0    851  \n",
       "18  0.516764    768.0    768  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default threshold (0.5) evaluation\n",
    "test_preds_default = (test_probs > 0.5).astype(int)\n",
    "\n",
    "report_default = classification_report(test_labels, test_preds_default, target_names=official_labels, output_dict=True)\n",
    "df_report_default = pd.DataFrame(report_default).transpose()\n",
    "\n",
    "# Calculate counts per label for reference\n",
    "all_clean_items = []\n",
    "for row in test_labels:\n",
    "    for i, val in enumerate(row):\n",
    "        if val == 1:\n",
    "            all_clean_items.append(official_labels[i])\n",
    "counts = pd.Series(all_clean_items).value_counts()\n",
    "\n",
    "out_default = (\n",
    "    df_report_default\n",
    "      .sort_values(\"f1-score\", ascending=False)\n",
    "      .assign(count=lambda d: d.index.map(counts).astype(\"Int64\"))\n",
    "      .reset_index(names=\"label\")\n",
    ")\n",
    "\n",
    "print(\"=== Test Set Performance (Default Threshold = 0.5) ===\\n\")\n",
    "out_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tagogidd5p",
   "metadata": {},
   "source": [
    "### Test Set Evaluation: Optimized Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "xnw8f7i8t1m",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Set Performance (Optimized Thresholds) ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crime and punishment</td>\n",
       "      <td>0.785127</td>\n",
       "      <td>0.838158</td>\n",
       "      <td>0.810776</td>\n",
       "      <td>2280.0</td>\n",
       "      <td>2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Legality, constitutionality and jurisprudence</td>\n",
       "      <td>0.780243</td>\n",
       "      <td>0.841155</td>\n",
       "      <td>0.809555</td>\n",
       "      <td>2216.0</td>\n",
       "      <td>2216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quality of life</td>\n",
       "      <td>0.757738</td>\n",
       "      <td>0.867631</td>\n",
       "      <td>0.808970</td>\n",
       "      <td>2765.0</td>\n",
       "      <td>2765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Economic</td>\n",
       "      <td>0.770196</td>\n",
       "      <td>0.844731</td>\n",
       "      <td>0.805744</td>\n",
       "      <td>2325.0</td>\n",
       "      <td>2325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Health and safety</td>\n",
       "      <td>0.768248</td>\n",
       "      <td>0.792844</td>\n",
       "      <td>0.780352</td>\n",
       "      <td>1593.0</td>\n",
       "      <td>1593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Political</td>\n",
       "      <td>0.738046</td>\n",
       "      <td>0.807920</td>\n",
       "      <td>0.771404</td>\n",
       "      <td>2197.0</td>\n",
       "      <td>2197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Security and defense</td>\n",
       "      <td>0.732958</td>\n",
       "      <td>0.811634</td>\n",
       "      <td>0.770292</td>\n",
       "      <td>1444.0</td>\n",
       "      <td>1444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Policy prescription and evaluation</td>\n",
       "      <td>0.715311</td>\n",
       "      <td>0.827927</td>\n",
       "      <td>0.767510</td>\n",
       "      <td>2528.0</td>\n",
       "      <td>2528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.720976</td>\n",
       "      <td>0.794646</td>\n",
       "      <td>0.754876</td>\n",
       "      <td>25960.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>micro avg</td>\n",
       "      <td>0.718655</td>\n",
       "      <td>0.794646</td>\n",
       "      <td>0.754743</td>\n",
       "      <td>25960.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>samples avg</td>\n",
       "      <td>0.739912</td>\n",
       "      <td>0.808065</td>\n",
       "      <td>0.750989</td>\n",
       "      <td>25960.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Cultural identity</td>\n",
       "      <td>0.718137</td>\n",
       "      <td>0.755155</td>\n",
       "      <td>0.736181</td>\n",
       "      <td>1552.0</td>\n",
       "      <td>1552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>0.704039</td>\n",
       "      <td>0.766836</td>\n",
       "      <td>0.732892</td>\n",
       "      <td>25960.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Fairness and equality</td>\n",
       "      <td>0.700633</td>\n",
       "      <td>0.741621</td>\n",
       "      <td>0.720545</td>\n",
       "      <td>1641.0</td>\n",
       "      <td>1641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Morality</td>\n",
       "      <td>0.682746</td>\n",
       "      <td>0.721569</td>\n",
       "      <td>0.701621</td>\n",
       "      <td>510.0</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Public opinion</td>\n",
       "      <td>0.598453</td>\n",
       "      <td>0.812411</td>\n",
       "      <td>0.689208</td>\n",
       "      <td>2095.0</td>\n",
       "      <td>2095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>External regulation and reputation</td>\n",
       "      <td>0.678245</td>\n",
       "      <td>0.620921</td>\n",
       "      <td>0.648318</td>\n",
       "      <td>1195.0</td>\n",
       "      <td>1195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Capacity and resources</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.613396</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>851.0</td>\n",
       "      <td>851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Other</td>\n",
       "      <td>0.513812</td>\n",
       "      <td>0.605469</td>\n",
       "      <td>0.555888</td>\n",
       "      <td>768.0</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            label  precision    recall  \\\n",
       "0                            Crime and punishment   0.785127  0.838158   \n",
       "1   Legality, constitutionality and jurisprudence   0.780243  0.841155   \n",
       "2                                 Quality of life   0.757738  0.867631   \n",
       "3                                        Economic   0.770196  0.844731   \n",
       "4                               Health and safety   0.768248  0.792844   \n",
       "5                                       Political   0.738046  0.807920   \n",
       "6                            Security and defense   0.732958  0.811634   \n",
       "7              Policy prescription and evaluation   0.715311  0.827927   \n",
       "8                                    weighted avg   0.720976  0.794646   \n",
       "9                                       micro avg   0.718655  0.794646   \n",
       "10                                    samples avg   0.739912  0.808065   \n",
       "11                              Cultural identity   0.718137  0.755155   \n",
       "12                                      macro avg   0.704039  0.766836   \n",
       "13                          Fairness and equality   0.700633  0.741621   \n",
       "14                                       Morality   0.682746  0.721569   \n",
       "15                                 Public opinion   0.598453  0.812411   \n",
       "16             External regulation and reputation   0.678245  0.620921   \n",
       "17                         Capacity and resources   0.620690  0.613396   \n",
       "18                                          Other   0.513812  0.605469   \n",
       "\n",
       "    f1-score  support  count  \n",
       "0   0.810776   2280.0   2280  \n",
       "1   0.809555   2216.0   2216  \n",
       "2   0.808970   2765.0   2765  \n",
       "3   0.805744   2325.0   2325  \n",
       "4   0.780352   1593.0   1593  \n",
       "5   0.771404   2197.0   2197  \n",
       "6   0.770292   1444.0   1444  \n",
       "7   0.767510   2528.0   2528  \n",
       "8   0.754876  25960.0   <NA>  \n",
       "9   0.754743  25960.0   <NA>  \n",
       "10  0.750989  25960.0   <NA>  \n",
       "11  0.736181   1552.0   1552  \n",
       "12  0.732892  25960.0   <NA>  \n",
       "13  0.720545   1641.0   1641  \n",
       "14  0.701621    510.0    510  \n",
       "15  0.689208   2095.0   2095  \n",
       "16  0.648318   1195.0   1195  \n",
       "17  0.617021    851.0    851  \n",
       "18  0.555888    768.0    768  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply optimized per-class thresholds\n",
    "test_preds_optimized = np.zeros_like(test_probs)\n",
    "for i in range(n_classes):\n",
    "    test_preds_optimized[:, i] = (test_probs[:, i] > best_thresholds[i]).astype(int)\n",
    "\n",
    "report_optimized = classification_report(test_labels, test_preds_optimized, target_names=official_labels, output_dict=True)\n",
    "df_report_optimized = pd.DataFrame(report_optimized).transpose()\n",
    "\n",
    "out_optimized = (\n",
    "    df_report_optimized\n",
    "      .sort_values(\"f1-score\", ascending=False)\n",
    "      .assign(count=lambda d: d.index.map(counts).astype(\"Int64\"))\n",
    "      .reset_index(names=\"label\")\n",
    ")\n",
    "\n",
    "print(\"=== Test Set Performance (Optimized Thresholds) ===\\n\")\n",
    "out_optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gaoyac1diqk",
   "metadata": {},
   "source": [
    "### Comparison with Previous Training Runs\n",
    "\n",
    "| Run | Model | Data | Micro F1 (Optimized) | Macro F1 (Optimized) |\n",
    "|-----|-------|------|---------------------|---------------------|\n",
    "| Run 1 | RoBERTa-base (Head+Tail) | All Topics | 0.731 | 0.706 |\n",
    "| Run 2 | RoBERTa-base (Weighted Loss) | All Topics | 0.729 | 0.706 |\n",
    "| Run 3 | RoBERTa-base (Politics Expert) | Politics Only | 0.758 | 0.686 |\n",
    "| **Longformer** | Longformer-base-4096 | All Topics + Topic Injection | 0.755 | 0.732 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8yazm28jmzi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LONGFORMER TEST SET SUMMARY\n",
      "============================================================\n",
      "\n",
      "Default Threshold (0.5):\n",
      "  Micro F1: 0.7419\n",
      "  Macro F1: 0.7218\n",
      "\n",
      "Optimized Thresholds:\n",
      "  Micro F1: 0.7547\n",
      "  Macro F1: 0.7329\n",
      "\n",
      "Improvement from threshold optimization:\n",
      "  Micro F1: +1.28%\n",
      "  Macro F1: +1.11%\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CROSS-RUN COMPARISON (Test Set, Optimized Thresholds)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run</th>\n",
       "      <th>Data</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Run 1 (RoBERTa)</td>\n",
       "      <td>All Topics</td>\n",
       "      <td>0.731000</td>\n",
       "      <td>0.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run 2 (Weighted)</td>\n",
       "      <td>All Topics</td>\n",
       "      <td>0.729000</td>\n",
       "      <td>0.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run 3 (Politics)</td>\n",
       "      <td>Politics Only</td>\n",
       "      <td>0.758000</td>\n",
       "      <td>0.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Longformer</td>\n",
       "      <td>All Topics + Topic</td>\n",
       "      <td>0.754743</td>\n",
       "      <td>0.732892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Run                Data  Micro F1  Macro F1\n",
       "0   Run 1 (RoBERTa)          All Topics  0.731000  0.706000\n",
       "1  Run 2 (Weighted)          All Topics  0.729000  0.706000\n",
       "2  Run 3 (Politics)       Politics Only  0.758000  0.686000\n",
       "3        Longformer  All Topics + Topic  0.754743  0.732892"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary metrics comparison\n",
    "micro_f1_default = f1_score(test_labels, test_preds_default, average='micro')\n",
    "macro_f1_default = f1_score(test_labels, test_preds_default, average='macro')\n",
    "\n",
    "micro_f1_optimized = f1_score(test_labels, test_preds_optimized, average='micro')\n",
    "macro_f1_optimized = f1_score(test_labels, test_preds_optimized, average='macro')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LONGFORMER TEST SET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDefault Threshold (0.5):\")\n",
    "print(f\"  Micro F1: {micro_f1_default:.4f}\")\n",
    "print(f\"  Macro F1: {macro_f1_default:.4f}\")\n",
    "print(f\"\\nOptimized Thresholds:\")\n",
    "print(f\"  Micro F1: {micro_f1_optimized:.4f}\")\n",
    "print(f\"  Macro F1: {macro_f1_optimized:.4f}\")\n",
    "print(f\"\\nImprovement from threshold optimization:\")\n",
    "print(f\"  Micro F1: +{(micro_f1_optimized - micro_f1_default)*100:.2f}%\")\n",
    "print(f\"  Macro F1: +{(macro_f1_optimized - macro_f1_default)*100:.2f}%\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Comparison with prior runs\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CROSS-RUN COMPARISON (Test Set, Optimized Thresholds)\")\n",
    "print(\"=\" * 60)\n",
    "comparison_data = {\n",
    "    'Run': ['Run 1 (RoBERTa)', 'Run 2 (Weighted)', 'Run 3 (Politics)', 'Longformer'],\n",
    "    'Data': ['All Topics', 'All Topics', 'Politics Only', 'All Topics + Topic'],\n",
    "    'Micro F1': [0.731, 0.729, 0.758, micro_f1_optimized],\n",
    "    'Macro F1': [0.706, 0.706, 0.686, macro_f1_optimized]\n",
    "}\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "df_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jbdawtam5yr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved optimized thresholds to: saved_models/framing_training_runs_longformer/20260121_0143_longformer_topic_expert_v1/class_thresholds_optimized.json\n",
      "Saved classification report to: saved_models/framing_training_runs_longformer/20260121_0143_longformer_topic_expert_v1/classification_report_optimized.csv\n",
      "\n",
      "Optimized Thresholds per Class:\n",
      "  Economic                                     : 0.35\n",
      "  Capacity and resources                       : 0.85\n",
      "  Morality                                     : 0.85\n",
      "  Fairness and equality                        : 0.55\n",
      "  Legality, constitutionality and jurisprudence: 0.40\n",
      "  Policy prescription and evaluation           : 0.40\n",
      "  Crime and punishment                         : 0.45\n",
      "  Security and defense                         : 0.60\n",
      "  Health and safety                            : 0.60\n",
      "  Quality of life                              : 0.40\n",
      "  Cultural identity                            : 0.70\n",
      "  Public opinion                               : 0.45\n",
      "  Political                                    : 0.50\n",
      "  External regulation and reputation           : 0.70\n",
      "  Other                                        : 0.70\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Save optimized thresholds and classification report\n",
    "import json\n",
    "\n",
    "# Save thresholds\n",
    "threshold_dict = dict(zip(official_labels, best_thresholds.tolist()))\n",
    "threshold_save_path = \"saved_models/framing_training_runs_longformer/20260121_0143_longformer_topic_expert_v1/class_thresholds_optimized.json\"\n",
    "\n",
    "with open(threshold_save_path, 'w') as f:\n",
    "    json.dump(threshold_dict, f, indent=4)\n",
    "print(f\"Saved optimized thresholds to: {threshold_save_path}\")\n",
    "\n",
    "# Save classification report\n",
    "report_save_path = \"saved_models/framing_training_runs_longformer/20260121_0143_longformer_topic_expert_v1/classification_report_optimized.csv\"\n",
    "out_optimized.to_csv(report_save_path, index=False)\n",
    "print(f\"Saved classification report to: {report_save_path}\")\n",
    "\n",
    "# Display final thresholds\n",
    "print(\"\\nOptimized Thresholds per Class:\")\n",
    "for label, thresh in threshold_dict.items():\n",
    "    print(f\"  {label:<45}: {thresh:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8xyi1jsl68q",
   "metadata": {},
   "source": [
    "### Longformer Run 1 - Evaluation Notes\n",
    "\n",
    "**Configuration:**\n",
    "- Model: `allenai/longformer-base-4096`\n",
    "- Max Length: 2048 tokens\n",
    "- Batch Size: 4 (effective 16 with gradient accumulation)\n",
    "- Training Data: All topics with `TOPIC:` prefix injection\n",
    "- Best Epoch: 3 (based on validation Micro F1)\n",
    "\n",
    "**Key Observations:**\n",
    "- Compare Longformer results against RoBERTa baseline (Run 1-2) to assess impact of full document context\n",
    "- Topic injection strategy aims to provide domain signal similar to Run 3's politics-only approach\n",
    "- Per-class threshold optimization continues to provide meaningful gains over default 0.5 threshold\n",
    "\n",
    "**Dataset Quality Ceiling:**\n",
    "Per CLAUDE.md analysis, the Mistral-generated labels have inherent noise (gold-standard F1 ~0.50). Models are effectively learning to replicate Mistral's labeling behavior rather than true frame detection. Best performance expected on \"reliable\" classes: Legality, Crime, Political."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d75fac4",
   "metadata": {},
   "source": [
    "### Action Plan: Optimizations from Wu et al. (2023)\n",
    "\n",
    "**1. Data Strategy: Include \"Negative\" Samples**\n",
    "\n",
    "* **Context:** Subtask 3 (Persuasion)\n",
    "* **Action:** Ensure your dataloader includes articles/segments where Mistral detected *no* frames (assigning them a `[0,0,...0]` label vector).\n",
    "* \n",
    "**Reasoning:** The paper found that including unlabelled paragraphs led to \"significant improvement\" compared to baselines that discarded them .\n",
    "\n",
    "\n",
    "\n",
    "**2. Pre-training: Implement TAPT (Task-Adaptive Pre-training)**\n",
    "\n",
    "* **Context:** Subtask 2 (Framing)\n",
    "* \n",
    "**Action:** Before fine-tuning for classification, train your Longformer on the *raw text* of your dataset using the Masked Language Modeling (MLM) objective (Paper used 60 epochs) .\n",
    "\n",
    "\n",
    "* **Reasoning:** This adapts the model's embeddings to the specific vocabulary of political framing (e.g., \"Rights\" in gun control vs. healthcare) prior to learning the labels.\n",
    "\n",
    "**3. Imbalance Handling: Switch to Oversampling**\n",
    "\n",
    "* **Context:** Subtask 1 (Genre) & Subtask 2 (Framing)\n",
    "* **Action:** Remove `pos_weight`. Instead, perform **Random Oversampling** (duplicate rows of rare classes).\n",
    "* \n",
    "**Reasoning:** The paper noted that class weights improved rare classes at the \"expense of more frequent classes\". Given your noisy Mistral labels, aggressive loss weighting may force the model to overfit hallucinations; oversampling was found to be slightly advantageous.\n",
    "\n",
    "\n",
    "\n",
    "**4. Architecture: Consider RoBERTa-MUPPET**\n",
    "\n",
    "* **Context:** Subtask 2 (Framing)\n",
    "* **Action:** If Longformer performance stalls, benchmark against `RoBERTa-MUPPET-Large`.\n",
    "* \n",
    "**Reasoning:** This model achieved 1st place in the English monolingual framing task, explicitly outperforming standard baselines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "framing_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
