{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9390b6b8",
   "metadata": {},
   "source": [
    "## Topic classifier using BERT V2\n",
    "\n",
    "V2 Version of topic classifier. Implementing several lessons from V1, including:\n",
    "* More robust, useful data loading to actually handle debugging, EDA\n",
    "* Implementing regularization, steps to avoid overfitting\n",
    "* Abandoning DistillBERT in favor of larger transformer model, specifically `FacebookAI/roberta-base` which is case-sensitive AND has larger parameters, which may aid in allowing the model to distinguish between topics more effectively\n",
    "* Roberta does not have, to my knowledge, a good fine-tuned model specifically for online article classification\n",
    "* Potentially increase total dataset size so rarer classes have more training examples\n",
    "\n",
    "In a separate notebook:\n",
    "* Trying modern community standards such as `deberta-v3-base-zeroshot-v2.0` that don't require labelled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "261757ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import transformers\n",
    "load_dotenv()  # looks for .env in current directory or parent\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8949d72d",
   "metadata": {},
   "source": [
    "#### Follow the same data preparation steps\n",
    "\n",
    "* Sample a dataset via query\n",
    "* Filter out low-N topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a68a0732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Column(name='gpt_topic', type_code=25), Column(name='maintext', type_code=25))\n"
     ]
    }
   ],
   "source": [
    "# Connect to server \n",
    "import psycopg2\n",
    "conn = psycopg2.connect(\n",
    "    dbname=os.getenv(\"DB_NAME\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    port=os.getenv(\"DB_PORT\")\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# key: set the seed\n",
    "cur.execute(\"SELECT setseed(0.42)\")\n",
    "\n",
    "# Do our join in database\n",
    "cur.execute(f\"\"\"\n",
    "           SELECT gpt_topic,\n",
    "           b.maintext\n",
    "           FROM mm_framing_full a\n",
    "           JOIN newsarticles b ON a.url = b.url\n",
    "           ORDER BY RANDOM()\n",
    "            LIMIT 30000\n",
    "            \"\"\")\n",
    "\n",
    "result= cur.fetchall()\n",
    "\n",
    "print(cur.description)\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "df = pd.DataFrame(result, columns=[\"gpt_topic\", \"article_text\"])\n",
    "df.head()\n",
    "\n",
    "del result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8174e8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 30000\n",
      "Filtered rows: 29561\n",
      "Topics removed: 71\n",
      "<bound method IndexOpsMixin.nunique of 0          Sports\n",
      "1         Economy\n",
      "2          Sports\n",
      "3          Sports\n",
      "4        Politics\n",
      "           ...   \n",
      "29995      Health\n",
      "29996    Business\n",
      "29997      Sports\n",
      "29998         War\n",
      "29999      Health\n",
      "Name: gpt_topic, Length: 29561, dtype: object>\n"
     ]
    }
   ],
   "source": [
    "## ESSENTIAL DATA FILTERING\n",
    "# REMOVE rows that have less than 10 observations for the topic\n",
    "\n",
    "counts = df.groupby('gpt_topic')['gpt_topic'].transform('count')\n",
    "\n",
    "# 2. Filter the DataFrame\n",
    "df_filtered = df[counts >= 10].copy()\n",
    "\n",
    "# 3. Check the result\n",
    "print(f\"Original rows: {len(df)}\")\n",
    "print(f\"Filtered rows: {len(df_filtered)}\")\n",
    "print(f\"Topics removed: {df['gpt_topic'].nunique() - df_filtered['gpt_topic'].nunique()}\")\n",
    "df = df_filtered\n",
    "\n",
    "del df_filtered, counts\n",
    "\n",
    "print(df['gpt_topic'].nunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bc8807a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidating labels to ~20 labels\n",
    "category_mapping = {\n",
    "    # 1. Politics & Global Affairs (Added 'Government')\n",
    "    'Politics': 'Politics',\n",
    "    'International Relations': 'Politics',\n",
    "    'International': 'Politics',\n",
    "    'Government': 'Politics',\n",
    "\n",
    "    # 2. Crime & Safety\n",
    "    'Crime': 'Crime & Safety',\n",
    "    'Safety': 'Crime & Safety',\n",
    "    'Public Safety': 'Crime & Safety',\n",
    "    'Security': 'Crime & Safety',\n",
    "    'Cybersecurity': 'Crime & Safety',\n",
    "\n",
    "    # 3. Legal\n",
    "    'Legal': 'Legal',\n",
    "\n",
    "    # 4. Business & Economy (Added 'Consumer')\n",
    "    'Business': 'Business & Economy',\n",
    "    'Economy': 'Business & Economy',\n",
    "    'Economics': 'Business & Economy',\n",
    "    'Finance': 'Business & Economy',\n",
    "    'Labor': 'Business & Economy',\n",
    "    'Consumer': 'Business & Economy',\n",
    "\n",
    "    # 5. Sports\n",
    "    'Sports': 'Sports',\n",
    "\n",
    "    # 6. Health\n",
    "    'Health': 'Health',\n",
    "    'Healthcare': 'Health',\n",
    "\n",
    "    # 7. Entertainment (Added 'Celebrity')\n",
    "    'Entertainment': 'Entertainment',\n",
    "    'Celebrity': 'Entertainment',\n",
    "\n",
    "    # 8. Lifestyle & Culture\n",
    "    'Culture': 'Lifestyle & Culture',\n",
    "    'Lifestyle': 'Lifestyle & Culture',\n",
    "    'Travel': 'Lifestyle & Culture',\n",
    "    'Food': 'Lifestyle & Culture',\n",
    "    'Arts': 'Lifestyle & Culture',\n",
    "    'Fashion': 'Lifestyle & Culture',\n",
    "    'History': 'Lifestyle & Culture',\n",
    "    'Religion': 'Lifestyle & Culture',\n",
    "    'Family': 'Lifestyle & Culture',\n",
    "    'Human Interest': 'Lifestyle & Culture',\n",
    "    'Obituary': 'Lifestyle & Culture',\n",
    "\n",
    "    # 9. Environment & Nature (Added 'Animals')\n",
    "    'Environment': 'Environment & Nature',\n",
    "    'Wildlife': 'Environment & Nature',\n",
    "    'Animal Rights': 'Environment & Nature',\n",
    "    'Animal': 'Environment & Nature',\n",
    "    'Animals': 'Environment & Nature',\n",
    "    'Animal Welfare': 'Environment & Nature',\n",
    "    'Agriculture': 'Environment & Nature',\n",
    "\n",
    "    # 10. War & Conflict\n",
    "    'War': 'War & Conflict',\n",
    "    'Military': 'War & Conflict',\n",
    "\n",
    "    # 11. Science & Technology\n",
    "    'Technology': 'Science & Technology',\n",
    "    'Science': 'Science & Technology',\n",
    "\n",
    "    # 12. Disaster & Accidents\n",
    "    'Accident': 'Disaster & Accidents',\n",
    "    'Accidents': 'Disaster & Accidents',\n",
    "    'Disaster': 'Disaster & Accidents',\n",
    "    'Disasters and Emergencies': 'Disaster & Accidents',\n",
    "    'Natural Disasters': 'Disaster & Accidents',\n",
    "    'Natural Disaster': 'Disaster & Accidents',\n",
    "    'Emergency': 'Disaster & Accidents',\n",
    "\n",
    "    # 13. Social Issues (Added 'Humanitarian')\n",
    "    'Social Issues': 'Social Issues',\n",
    "    'Social': 'Social Issues',\n",
    "    'Society': 'Social Issues',\n",
    "    'Community': 'Social Issues',\n",
    "    'Charity': 'Social Issues',\n",
    "    'Human Rights': 'Social Issues',\n",
    "    'Humanitarian': 'Social Issues',\n",
    "\n",
    "    # 14. Infrastructure & Transport\n",
    "    'Transportation': 'Infrastructure & Transport',\n",
    "    'Infrastructure': 'Infrastructure & Transport',\n",
    "    'Housing': 'Infrastructure & Transport',\n",
    "    'Urban Development': 'Infrastructure & Transport',\n",
    "\n",
    "    # 15. Independent Mid-Sized Categories\n",
    "    'Weather': 'Weather',\n",
    "    'Education': 'Education',\n",
    "    'Media': 'Media',\n",
    "    'Immigration': 'Immigration',\n",
    "\n",
    "    # 16. Handling Nulls/Other\n",
    "    'no_topic': 'Other/Unknown',\n",
    "    'No_topic': 'Other/Unknown'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "80e4389f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_topic\n",
      "Politics                      4522\n",
      "Crime & Safety                4176\n",
      "Sports                        3772\n",
      "Business & Economy            2406\n",
      "Legal                         1922\n",
      "Lifestyle & Culture           1867\n",
      "Health                        1481\n",
      "Environment & Nature          1400\n",
      "Entertainment                 1343\n",
      "War & Conflict                 911\n",
      "Social Issues                  905\n",
      "Disaster & Accidents           720\n",
      "Science & Technology           685\n",
      "Education                      684\n",
      "Weather                        641\n",
      "Immigration                    627\n",
      "Media                          601\n",
      "Other/Unknown                  522\n",
      "Infrastructure & Transport     376\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count        29561\n",
       "unique          19\n",
       "top       Politics\n",
       "freq          4522\n",
       "Name: cleaned_topic, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['topic_cat'] = df['gpt_topic'].astype('category')\n",
    "\n",
    "# Create the new column\n",
    "df['cleaned_topic'] = df['topic_cat'].map(category_mapping)\n",
    "\n",
    "# IMPORTANT: .map() converts anything NOT in the dictionary to NaN (missing).\n",
    "# Convert to other/unknown for simplicity, for these edge cases\n",
    "df['cleaned_topic'] = df['cleaned_topic'].fillna('Other/Unknown')\n",
    "\n",
    "# set dtype\n",
    "df['cleaned_topic'] = df['cleaned_topic'].astype('category')\n",
    "\n",
    "# Check your new reduced value counts\n",
    "print(df['cleaned_topic'].value_counts())\n",
    "\n",
    "df['cleaned_topic'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "485cf91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rereate label_ids mapping (string --> int)\n",
    "label_ids = df['cleaned_topic'].cat.codes\n",
    "id_to_label = list(df['cleaned_topic'].cat.categories)\n",
    "label_to_id = {label: i for i, label in enumerate(id_to_label)}\n",
    "#label_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6eac2544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gpt_topic', 'article_text', 'topic_cat', 'cleaned_topic'], dtype='object')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87761fcf",
   "metadata": {},
   "source": [
    "### Tokenize + Apply custom head/tail truncation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "44951034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1492 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# choosing a base tokenizer model, this won't \n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# generally want to keep the tokenizations in a separate variable rather than adding them back to the dataframe (for efficiency, taking advantage of the batching features of the huggingface tokenizer)\n",
    "\n",
    "encodings = tokenizer(df['article_text'].tolist(),                      # we opt to dynamically do padding later with head/tail strategy\n",
    "                      # hence we also don't need truncation here\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "237f867d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 133, 764] 512\n",
      "[0, 1106, 47] 512\n",
      "[0, 133, 188] 38\n",
      "[0, 243, 16] 512\n",
      "[0, 27822, 1862] 512\n",
      "[0, 2571, 20606] 481\n",
      "[0, 725, 3765] 235\n",
      "[0, 13716, 1247] 512\n",
      "[0, 32828, 4127] 265\n",
      "[0, 5341, 1185] 116\n"
     ]
    }
   ],
   "source": [
    "# Code to set the unique head and tail of the articles\n",
    "head_len = 320\n",
    "tail_len = 190\n",
    "content_len = head_len + tail_len          \n",
    "max_len = content_len + 2  # [CLS] + [SEP]      \n",
    "\n",
    "cls_id = tokenizer.cls_token_id\n",
    "sep_id = tokenizer.sep_token_id\n",
    "pad_id = tokenizer.pad_token_id\n",
    "\n",
    "for i, ids in enumerate(encodings[\"input_ids\"]):\n",
    "    # ClS and Sep Id's are present in tokenized values, so do this\n",
    "    if len(ids) >= 2 and ids[0] == cls_id and ids[-1] == sep_id:\n",
    "        ids = ids[1:-1]\n",
    "\n",
    "    # Head+tail on the content tokens\n",
    "    if len(ids) > content_len:\n",
    "        head = ids[:head_len]\n",
    "        tail = ids[-tail_len:]\n",
    "        ids = head + tail\n",
    "\n",
    "    # Add specials back\n",
    "    ids = [cls_id] + ids + [sep_id]\n",
    "\n",
    "    # Build attention mask (1 for real tokens)\n",
    "    mask = [1] * len(ids)\n",
    "\n",
    "    # Pad (or truncate)\n",
    "    if len(ids) < max_len:\n",
    "        pad_n = max_len - len(ids)\n",
    "        ids = ids + [pad_id] * pad_n\n",
    "        mask = mask + [0] * pad_n\n",
    "    else:\n",
    "        ids = ids[:max_len]\n",
    "        mask = mask[:max_len]\n",
    "\n",
    "    # Write back\n",
    "    encodings[\"input_ids\"][i] = ids\n",
    "    encodings[\"attention_mask\"][i] = mask\n",
    "\n",
    "# quick sanity check\n",
    "for ids, mask in zip(encodings[\"input_ids\"][:10], encodings[\"attention_mask\"][:10]):\n",
    "    assert len(ids) == 512 and len(mask) == 512\n",
    "    print(ids[:3], sum(mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396c6e01",
   "metadata": {},
   "source": [
    "### PyTorch Dataset Creation\n",
    "\n",
    "Using `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`\n",
    "\n",
    "[Pytorch data tutorial](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c22c7b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying the below to record the raw text, but filter this out in batch processing since it's not a tensor\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsArticleDataset(Dataset):\n",
    "    def __init__(self, encodings, label_ids: pd.Series, article_text: pd.Series):\n",
    "        self.input_ids = encodings['input_ids']\n",
    "        self.attention_mask = encodings['attention_mask']\n",
    "        self.labels = label_ids.to_list() # convert to list for indexing robustness \n",
    "        self.article_text =  article_text.to_list() # convert as well\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        # for transformer style training, generally return a dict with keys that match what your model expects\n",
    "        dict_to_return = {'input_ids': torch.tensor(self.input_ids[idx], dtype = torch.long), # convert to tensors, 64-bit integer (long) dtype \n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype = torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype =torch.long),\n",
    "            'article_text': self.article_text[idx]} # no need to convert text to tensor\n",
    "        return dict_to_return\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cb52f7",
   "metadata": {},
   "source": [
    "### Creating the test/train/val split\n",
    "\n",
    "Since we have so many unique classes (100+) it seems best to stratify our data split to balance class distributions.\n",
    "\n",
    "Can use `sklearn.model_selection.train_test_split`\n",
    "\n",
    "This finds the indexes for the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "afde4114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23648 2956 2957\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# integer-coded labels aligned with encodings order\n",
    "labels = np.array(label_ids, dtype=np.int64) # numpy array\n",
    "\n",
    "N = len(labels)\n",
    "idx = np.arange(N)\n",
    "\n",
    "# 80/10/10 split (train/val/test), stratified\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    idx,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=labels # as array\n",
    ")\n",
    "\n",
    "# Run it again on the temp test set to get the val set\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx,\n",
    "    test_size=0.50,                    # half of 20% -> 10% val, 10% test\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=labels[temp_idx]\n",
    ")\n",
    "\n",
    "# Build ONE dataset, then slice with Subset\n",
    "full_df = NewsArticleDataset(encodings, label_ids, df['article_text'])  # or NewsArticleDataset(encodings, labels)\n",
    "train = Subset(full_df, train_idx) # basically does what it says on the tin\n",
    "val   = Subset(full_df, val_idx)\n",
    "test  = Subset(full_df, test_idx)\n",
    "\n",
    "print(len(train), len(val), len(test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9d754c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate the data loader, they rely on dataset + indices\n",
    "batch_size = 16\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# data loaders handle our raw (non-tensor) article text automatically\n",
    "train_loader = DataLoader(train, \n",
    "                          batch_size = batch_size, # number of articles to be fed into the model at once\n",
    "                          num_workers=0, # increase this later on\n",
    "                          shuffle = True, \n",
    "                          pin_memory= True)\n",
    "val_loader = DataLoader(val, \n",
    "                          batch_size = batch_size, # number of articles to be fed into the model at once\n",
    "                          shuffle = False, # false so eval is deterministic and reproducible\n",
    "                          pin_memory= True)\n",
    "test_loader = DataLoader(test, \n",
    "                          batch_size = batch_size, # number of articles to be fed into the model at once\n",
    "                          shuffle = False,  # false, as above\n",
    "                          pin_memory= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2c24ec35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 26931,   387,  ...,     1,     1,     1],\n",
      "        [    0, 26402, 45855,  ...,     1,     1,     1],\n",
      "        [    0,  5762,    36,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,   495,  1496,  ...,    17,    46,     2],\n",
      "        [    0,  9497,    26,  ...,     1,     1,     1],\n",
      "        [    0,  5341,  4180,  ...,  1716,     4,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([16,  2, 13, 13,  8, 13,  0,  1, 13, 11, 16, 13, 16, 13,  3,  0,  4, 16,\n",
      "         4, 18, 10, 13, 10,  6,  0,  0,  0,  1, 12,  5,  1, 17])}\n"
     ]
    }
   ],
   "source": [
    "# grab a batch using iterator next()\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "text = batch.pop(\"article_text\", None)\n",
    "\n",
    "print(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eecad1",
   "metadata": {},
   "source": [
    "### Begin training loop\n",
    "\n",
    "Overall training notes (unlike last notebook, not gonna keep a bunch of version of different training loops. Will just note my observations below, changing stuff flexibly)\n",
    "\n",
    "#### Training Run 1 - 15 epochs - 3e-5 LR - 30,000 N \n",
    "* Took 106 minutes, val accuracy peaked at 0.74\n",
    "Epoch 7 | train_loss=0.1688 | val_loss=1.2457 | val_acc=0.7439\n",
    "\n",
    "* Having examined the incorrect predictions, I'm feeling significant empathy for the model. In these cases, the 'true label' is quite debatable, and it feels like both can fit for the article. It doesn't appear like anything is deeply wrong with the training parameters themselves, so much as it's a data-side issue. \n",
    "* A key trend is that the model has difficulty with short-length predictions, I will try parsing these out earlier in the pipeline\n",
    "* Where the model does start to falter is in understanding moral nuances, for example an article about the killing of a grizzly bear was classified as: True label: Environment & Nature -- Pred label Crime & Safety, which is understandable but ideally this distinction would be picked up.\n",
    "\n",
    "Next steps: remove short articles (under 60 words) from training, increase N\n",
    "Epochs don't seem to help much, so keep at 3 for now. For fast iteration/testing\n",
    "\n",
    "#### Training Run 2 - 3 epochs - 3e-5 LR - 60,000 N\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "082ff31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a clean state, hard reset the GPU state\n",
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1c8ccf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model and head \n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(df['cleaned_topic'].unique()))\n",
    "model.to('cuda')\n",
    "\n",
    "# set optimizer and LR\n",
    "optimizer = torch.optim.AdamW(lr=3e-5, # for transformers, we want a low learning rate, even lower than 0.001\n",
    "                              weight_decay= 0.01, # this basically is L2 regularization applied through, penalizeing large weight, encouragin params to stay smaller, can help reduce overfitting and improve generalization\n",
    "                             params=model.parameters()) # tell the optimizer what parameters to optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cbf04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | train_loss=1.0665 | val_loss=0.8514 | val_acc=0.7341\n",
      "Epoch 1 | train_loss=0.7128 | val_loss=0.8366 | val_acc=0.7419\n",
      "Epoch 2 | train_loss=0.5483 | val_loss=0.8869 | val_acc=0.7368\n",
      "Epoch 3 | train_loss=0.4104 | val_loss=0.9764 | val_acc=0.7307\n",
      "Epoch 4 | train_loss=0.3267 | val_loss=1.0611 | val_acc=0.7334\n",
      "Epoch 5 | train_loss=0.2474 | val_loss=1.1169 | val_acc=0.7338\n",
      "Epoch 6 | train_loss=0.1977 | val_loss=1.1791 | val_acc=0.7283\n",
      "Epoch 7 | train_loss=0.1688 | val_loss=1.2457 | val_acc=0.7439\n",
      "Epoch 8 | train_loss=0.1373 | val_loss=1.2919 | val_acc=0.7294\n",
      "Epoch 9 | train_loss=0.1184 | val_loss=1.3663 | val_acc=0.7371\n",
      "Epoch 10 | train_loss=0.1089 | val_loss=1.4259 | val_acc=0.7179\n",
      "Epoch 11 | train_loss=0.1009 | val_loss=1.3580 | val_acc=0.7341\n",
      "Epoch 12 | train_loss=0.0932 | val_loss=1.4837 | val_acc=0.7223\n",
      "Epoch 13 | train_loss=0.0803 | val_loss=1.5678 | val_acc=0.7209\n",
      "Epoch 14 | train_loss=0.0823 | val_loss=1.5205 | val_acc=0.7426\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        #0) pop out the article text\n",
    "        article_text = batch.pop(\"article_text\", None)\n",
    "        \n",
    "        # 1) move batch to GPU\n",
    "        batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "\n",
    "        # 2) forward pass \n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"labels\"] # had a rough comma here b4\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss # default loss from the HF auto model, likely cross entropy\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 3) zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4) backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5) step\n",
    "        optimizer.step()\n",
    "\n",
    "    # ---- validation ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            #pop out text\n",
    "            article_text = batch.pop(\"article_text\", None)\n",
    "            \n",
    "            batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "            outputs = model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                labels=batch[\"labels\"],\n",
    "            )\n",
    "            val_loss += outputs.loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == batch[\"labels\"]).sum().item()\n",
    "            total += batch[\"labels\"].size(0)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch} | train_loss={running_loss/len(train_loader):.4f} \"\n",
    "        f\"| val_loss={val_loss/len(val_loader):.4f} | val_acc={correct/total:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98b8b49",
   "metadata": {},
   "source": [
    "### Test set evaluation/ exploration of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcfb2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Error analysis on test set\n",
    "\n",
    "test_results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        article_text = batch.pop(\"article_text\")\n",
    "        batch = {k: v.to(\"cuda\") for k, v in batch.items()}        \n",
    "        outputs = model(**batch)       \n",
    "        \n",
    "        # Get predictions\n",
    "        preds = outputs.logits.argmax(dim=-1)\n",
    "        \n",
    "        # Iterate through the batch to pair text with predictions\n",
    "        # We move tensors back to CPU (.cpu()) and convert to python scalars (.item())\n",
    "        current_labels = batch[\"labels\"].cpu().numpy()\n",
    "        current_preds = preds.cpu().numpy()\n",
    "        \n",
    "        for article_text, true_label, pred_label in zip(article_text, current_labels, current_preds):\n",
    "            test_results.append({\n",
    "                \"article_text\": article_text,\n",
    "                \"true_label\": true_label,\n",
    "                \"pred_label\": pred_label,\n",
    "                \"is_correct\": true_label == pred_label\n",
    "            })\n",
    "         \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ff48a2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Business & Economy': 0,\n",
       " 'Crime & Safety': 1,\n",
       " 'Disaster & Accidents': 2,\n",
       " 'Education': 3,\n",
       " 'Entertainment': 4,\n",
       " 'Environment & Nature': 5,\n",
       " 'Health': 6,\n",
       " 'Immigration': 7,\n",
       " 'Infrastructure & Transport': 8,\n",
       " 'Legal': 9,\n",
       " 'Lifestyle & Culture': 10,\n",
       " 'Media': 11,\n",
       " 'Other/Unknown': 12,\n",
       " 'Politics': 13,\n",
       " 'Science & Technology': 14,\n",
       " 'Social Issues': 15,\n",
       " 'Sports': 16,\n",
       " 'War & Conflict': 17,\n",
       " 'Weather': 18}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8014462f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_text</th>\n",
       "      <th>true_label</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>is_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEWYou can now listen to Fox News articles!\\nF...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Politics</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pat Sajak's \"Wheel of Fortune\" retirement anno...</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Florida Republican Sen. Rick Scott endorsed fo...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>Politics</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A woman reported being groped by someone along...</td>\n",
       "      <td>Crime &amp; Safety</td>\n",
       "      <td>Crime &amp; Safety</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A dog named Bobi, who'd been deemed the world'...</td>\n",
       "      <td>Lifestyle &amp; Culture</td>\n",
       "      <td>Lifestyle &amp; Culture</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_text           true_label  \\\n",
       "0  NEWYou can now listen to Fox News articles!\\nF...             Politics   \n",
       "1  Pat Sajak's \"Wheel of Fortune\" retirement anno...        Entertainment   \n",
       "2  Florida Republican Sen. Rick Scott endorsed fo...             Politics   \n",
       "3  A woman reported being groped by someone along...       Crime & Safety   \n",
       "4  A dog named Bobi, who'd been deemed the world'...  Lifestyle & Culture   \n",
       "\n",
       "            pred_label  is_correct  \n",
       "0             Politics        True  \n",
       "1        Entertainment        True  \n",
       "2             Politics        True  \n",
       "3       Crime & Safety        True  \n",
       "4  Lifestyle & Culture        True  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe\n",
    "df_results = pd.DataFrame(test_results)\n",
    "\n",
    "# create key to label mapping\n",
    "id_to_label_dict = {id: label for label, id in label_to_id.items()}\n",
    "\n",
    "# Convert label_ids back to true categories\n",
    "df_results['true_label'] = df_results['true_label'].map(id_to_label_dict)\n",
    "df_results['pred_label'] = df_results['pred_label'].map(id_to_label_dict)\n",
    "\n",
    "\n",
    "\n",
    "# id_to_label = list(df['cleaned_topic'].cat.categories)\n",
    "\n",
    "false_preds = df_results.query('is_correct == False').reset_index()\n",
    "\n",
    "df_results.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5de9a682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A grizzly bear was found dead near Yellowstone National Park this week and wildlife officials are investigating if the mammal was shot to death.\n",
      "The dead bear was found less than 30 yards off the roadside Tuesday in Cody, a city 14 miles outside the national park along the North Fork.\n",
      "Heartbreaking photos of the bloodied bear were captured by wildlife photographer Amy Gerber.\n",
      "“We are the villains in this story,” Gerber said on Facebook.\n",
      "“I will never understand the mindset of someone who, practically from the road, would pull out a gun and shoot a wild grizzly bear. For no apparent reason. Mistaken identity? I think not. This bear was huge….would be very difficult to mistake for a black bear. Self-defense? Again, I say no.”\n",
      "Gerber had seen the 500-pound mammal perusing the Shoshone River multiple times in the weeks since spring weather hit the area.\n",
      "She said the bear died less than a mile from where she had last seen him just a week earlier.\n",
      "There was initial speculation that the bear was hit by a car, but sources told Gerber — who witnessed US Fish and Wildlife Service officials searching the area for “evidence” — that the grizzly was fatally shot.\n",
      "The USFWS confirmed to The Post it is probing the apparent murder, but said it could not provide any details “due to the nature of ongoing investigations.”\n",
      "Grizzlies are classified as an endangered species and are federally protected.\n",
      "The maximum penalty for killing a grizzly is a $50,000 fine and up to a year in jail unless one can prove they were acting in self-defense.\n",
      "The alleged killing comes just three months after the Biden administration took steps to end federal protections for grizzly bears in the northern Rocky Mountains, which would open the door to future hunting in Montana, Wyoming and Idaho.\n",
      "Grizzly hunting was blocked in 2018 in Wyoming as populations started to dwindle, but officials argued the bears have made a substantial return and no longer need federal protections.\n",
      "======================================\n",
      "True label: Environment & Nature -- Pred label Crime & Safety\n"
     ]
    }
   ],
   "source": [
    "index = 491\n",
    "\n",
    "print(false_preds.iloc[index]['article_text'])\n",
    "print(\"======================================\")\n",
    "print(f\"True label: {false_preds.iloc[index]['true_label']} -- Pred label {false_preds.iloc[index]['pred_label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1a4afe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
