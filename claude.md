# Project Context: Frame Divergence Analyzer

## 1. The Goal
To build a lightweight, real-time tool that quantifies "Framing Bias" between news sources.
* **Input:** Two URLs (e.g., CNN vs. Fox News on the same topic).
* **Process:** Scrape text -> Classify into 15 Political Frames -> Calculate "Frame Delta."
* **Output:** "Source A focuses on *Morality* (60%), Source B focuses on *Economics* (40%)."

## 2. The Architecture
* **Model:** `allenai/longformer-base-4096` with topic injection (production model).
* **Task:** Multi-Label Text Classification (15 specific political frames).
* **Hardware:** Single GPU (16GB VRAM).
* **Best Performance:** Micro F1 0.755, Macro F1 0.733 (with optimized per-class thresholds).

## 3. The Dataset
* **Source:** `copenlu/mm-framing` (Hugging Face).
* **Method:** We use the *text* portion of this dataset.
* **Labels:** The 15 standard Media Frames Corpus labels (Economic, Capacity, Morality, etc.).
* **Note:** The dataset labels were generated by Mistral-7B; we are "distilling" them into DistilBERT.

## Dataset variables

Column name descriptions:
uuid - Unique ID for each article
title - Title of the article
date_publish - Publication date
source_domain - Domain of the publisher
url - Article URL
political_leaning - Political leaning of the publisher

---------- Annotations ---------------

text-topic - Article topic generated from article text
text-topic-exp - Article topic explanation
text-entity-name - Main entity in article text
text-entity-sentiment - Sentiment towards main entity
text-entity-sentiment-exp - Explanation of text sentiment
text-generic-frame - Generic Frame used in Article text
text-generic-frame-exp - Generic Frame in text explanation
text-issue-frame - Issue Frame used in article text
text-issue-frame-exp - Issue Frame explanation
img-generic-frame - Generic Frame used in Article Image
img-frame-exp - Generic Frame in image explanation
img-entity-name - Main subject in Article Image
img-entity-sentiment - Sentiment towards the subject in Article image
img-entity-sentiment-exp - Explanation of image sentiment
gpt-topic - Consolidated topic

## Down-stream evaluation

After framing prediction model is completed, then we might evaluate it on a key real world benchmark- the 

## How was the data above collected? Is it good data?

Authors used Mistral-7B-Instruct-v0.3 available via huggingface 5
with vLLM for high-throughput and memory-efficient
inference and set the parameters as temperature=0.2, max_tokens=4000 dtype=’half’ and max_model_len=8096. In essence they used prompt engineering to instruct the model to read the text and fit classifications into set frames. The original model performance is as follows, testing against the gold-standard media frames corpus.

| Label        | Precision | Recall | F1-score |
| ------------ | --------: | -----: | -------: |
| cap&res      |      0.39 |   0.34 |     0.36 |
| crime        |      0.50 |   0.87 |     0.63 |
| culture      |      0.38 |   0.37 |     0.37 |
| economic     |      0.43 |   0.69 |     0.53 |
| fairness     |      0.17 |   0.74 |     0.28 |
| health       |      0.48 |   0.48 |     0.48 |
| legality     |      0.53 |   0.87 |     0.66 |
| morality     |      0.30 |   0.63 |     0.41 |
| policy       |      0.40 |   0.73 |     0.51 |
| political    |      0.68 |   0.53 |     0.60 |
| public_op    |      0.32 |   0.55 |     0.40 |
| quality_life |      0.28 |   0.36 |     0.31 |
| regulation   |      0.26 |   0.48 |     0.34 |
| security     |      0.30 |   0.45 |     0.36 |
| micro avg    |      0.42 |   0.62 |     0.50 |
| macro avg    |      0.39 |   0.58 |     0.45 |
| weighted avg |      0.45 |   0.62 |     0.51 |
| samples avg  |      0.46 |   0.63 |     0.51 |

## Dataset Quality Analysis

**The Ceiling Problem:** Our distilled models are measured against Mistral's labels, not the gold standard. Since Mistral itself only achieves 0.50 F1 against the Media Frames Corpus, we're learning to replicate Mistral's labeling behavior.

**Longformer + Topic Injection Breakthrough:** Despite the label noise ceiling, Longformer with topic injection achieved meaningful gains over RoBERTa:
- Micro F1: 0.755 (+2.4% vs RoBERTa generalist)
- Macro F1: 0.733 (+2.7% vs RoBERTa generalist)
- Wins 10/15 classes vs the politics-only expert model

**Why It Works:** Topic injection (`TOPIC: Politics\n...`) provides domain signal that helps the model resolve frame ambiguity. Full document context captures frames that appear in mid-article (Quality of Life +23%, Health +21%).

**Per-Class Reliability (based on Mistral's gold-standard performance):**
- **Reliable:** Legality (0.66), Crime (0.63), Political (0.60)
- **Moderate:** Economic (0.53), Policy (0.51), Health (0.48)
- **Unreliable:** Fairness (0.28), Quality of Life (0.31), Regulation (0.34), Security (0.36), Cap & Res (0.36), Culture (0.37), Morality (0.41), Public Opinion (0.40)

**Implication for Downstream Use:** For comparing *relative* framing differences between sources (CNN vs Fox), systematic biases may cancel out. The Longformer model's improved Macro F1 means more balanced detection across all frame types.


## assistance instructions

Always avoid emojis

I'm working in a conda environment called torch-gpu, when running bash commands, firstly please run:
conda activate torch-gpu

this will activate torch-gpu in your own environment

avoid creating uni-task scripts, including scripts for minute technical tasks. If you do, then please delete them after they have served their purpose.


## Framing Classifier Work

**Objective:** Develop a lightweight multi-label classification model to detect 15 generic media frames (e.g., "Economic," "Fairness," "Legality") for social network analysis.

**Production Architecture (Longformer + Topic Injection):**

* **Base Model:** `allenai/longformer-base-4096`
* **Input Strategy:** Full text (max 2048 tokens) with `TOPIC:{topic}\n` prefix injection
* **Loss Function:** `BCEWithLogitsLoss` with `pos_weight` for class imbalance
* **Inference:** Per-class optimized thresholds (range 0.35-0.85)
* **Performance:** Micro F1 0.755, Macro F1 0.733

**Key Experiments & Findings:**

* **Run 1 - RoBERTa Baseline:** Head+Tail truncation (510 tokens) achieved 0.731 Micro F1, 0.706 Macro F1.
* **Run 2 - Weighted Loss:** No improvement over baseline. Recall +1.6%, Precision -1.0%, F1 unchanged.
* **Run 3 - Politics Expert:** Domain-specific training hit 0.758 Micro F1 but poor Macro F1 (0.686). Proved frames are context-dependent.
* **Run 4 - Longformer + Topic Injection:** Best overall for balanced performance. 0.755 Micro F1, 0.733 Macro F1. Wins 10/15 classes vs Run 3. Massive gains on rare classes (Quality of Life +23%, Health +21%, Capacity +14%).
* **Run 5 - RoBERTa + Topic Injection (Ablation):** Applied topic injection back to RoBERTa with N=150k (all topics). Achieved 0.758 Micro F1, 0.686 Macro F1. Matches Longformer on Micro F1 but 4.7% worse on Macro F1, confirming full document context is critical for rare class detection.

**Why Longformer + Topic Injection Works:**

1. **Topic prefix** provides domain signal without needing separate models
2. **Full context** captures mid-article framing (health details, lifestyle impacts)
3. **Better calibration** - higher thresholds indicate more confident predictions
4. **Single model** handles all topics with near-expert performance

**Run 5 Ablation Insights (RoBERTa + Topic Injection):**

Run 5 isolates the contribution of topic injection from full document context:

| Factor | Micro F1 Impact | Macro F1 Impact |
|--------|----------------|-----------------|
| Topic injection alone (Run 5 vs Run 1) | +2.7% (0.731 -> 0.758) | -2.0% (0.706 -> 0.686) |
| Full context (Longformer vs RoBERTa+Topic) | -0.3% | +4.7% |

**Conclusion:** Topic injection drives Micro F1 gains (overall accuracy), but Longformer's full document context is essential for Macro F1 (rare class detection). For production use where balanced frame detection matters, Longformer remains the better choice despite being heavier.

**Model Artifacts:**
* Weights: `notebooks/saved_models/framing_training_runs_longformer/20260121_0143_longformer_topic_expert_v1/model_ep3.bin`
* Thresholds: `notebooks/saved_models/framing_training_runs_longformer/20260121_0143_longformer_topic_expert_v1/class_thresholds_optimized.json`


## Link to a new test set - Sem Eval Task 3
https://propaganda.math.unipd.it/semeval2023task3/teampage.php?passcode=2e9cfe6444bc6c23b3a19209ea58ba6f
