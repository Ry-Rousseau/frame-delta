# Project Context: Frame Divergence Analyzer

## 1. The Goal
To build a lightweight, real-time tool that quantifies "Framing Bias" between news sources.
* **Input:** Two URLs (e.g., CNN vs. Fox News on the same topic).
* **Process:** Scrape text -> Classify into 15 Political Frames -> Calculate "Frame Delta."
* **Output:** "Source A focuses on *Morality* (60%), Source B focuses on *Economics* (40%)."

## 2. The Architecture
* **Model:** `distilbert-base-uncased` (Fine-tuned for Multi-Label Classification).
* **Task:** Multi-Label Text Classification (15 specific political frames).
* **Hardware:** Single GPU (16GB VRAM).

## 3. The Dataset
* **Source:** `copenlu/mm-framing` (Hugging Face).
* **Method:** We use the *text* portion of this dataset.
* **Labels:** The 15 standard Media Frames Corpus labels (Economic, Capacity, Morality, etc.).
* **Note:** The dataset labels were generated by Mistral-7B; we are "distilling" them into DistilBERT.

## Dataset variables

Column name descriptions:
uuid - Unique ID for each article
title - Title of the article
date_publish - Publication date
source_domain - Domain of the publisher
url - Article URL
political_leaning - Political leaning of the publisher

---------- Annotations ---------------

text-topic - Article topic generated from article text
text-topic-exp - Article topic explanation
text-entity-name - Main entity in article text
text-entity-sentiment - Sentiment towards main entity
text-entity-sentiment-exp - Explanation of text sentiment
text-generic-frame - Generic Frame used in Article text
text-generic-frame-exp - Generic Frame in text explanation
text-issue-frame - Issue Frame used in article text
text-issue-frame-exp - Issue Frame explanation
img-generic-frame - Generic Frame used in Article Image
img-frame-exp - Generic Frame in image explanation
img-entity-name - Main subject in Article Image
img-entity-sentiment - Sentiment towards the subject in Article image
img-entity-sentiment-exp - Explanation of image sentiment
gpt-topic - Consolidated topic

## Down-stream evaluation

After framing prediction model is completed, then we might evaluate it on a key real world benchmark- the 


## assistance instructions

Always avoid emojis

I'm working in a conda environment called torch-gpu, when running bash commands, firstly please run:
conda activate torch-gpu

this will activate torch-gpu in your own environment

avoid creating uni-task scripts, including scripts for minute technical tasks. If you do, then please delete them after they have served their purpose.


## Framing Classifier Work

**Objective:** Develop a lightweight multi-label classification model to detect 15 generic media frames (e.g., "Economic," "Fairness," "Legality") for social network analysis.

**Current Architecture:**

* **Base Model:** `roberta-base` (initially experimented with) transitioning to `allenai/longformer-base-4096`.
* **Input Strategy:** Moved from "Head+Tail" truncation (first 320 + last 190 tokens) to full-text input via Longformer to capture mid-document context.
* **Loss Function:** `BCEWithLogitsLoss` utilizing `pos_weight` to penalize missing rare classes (handling 1:50 class imbalances).

**Key Experiments & Findings: from the framing_classier.ipynb**

* **Baseline (Run 1):** `roberta-base` with truncation plateaued at **~0.73 Micro F1**. Post-training threshold optimization (tuning decision boundaries per class) yielded a ~1.1% global performance gain.
* **Weighted Loss (Run 2):** Implementing class weights improved Recall significantly (+1.6%) but hurt Precision. Overall F1 remained tied with the baseline, suggesting an information bottleneck in the input data.
* **Mixture of Experts Hypothesis (Run 3):** A test run training only on "Politics" articles showed massive performance jumps in nuanced frames (**Legality +8%**, **Fairness +8%**). This proved that frame definitions are context-dependent (e.g., "Fairness" means different things in Sports vs. Politics).

**Current Strategy:**

* **Topic Injection:** To operationalize the "Mixture of Experts" gains without maintaining multiple models, we are injecting the `gpt_topic` metadata directly into the input text (e.g., `"[TOPIC: Politics] Article text..."`). This provides the domain signal needed to resolve frame ambiguity.
* **Longformer Implementation:** Adopting Longformer with **Global Attention** set on the `[CLS]` token and injected topic tokens to aggregate context from the entire document.