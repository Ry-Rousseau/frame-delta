# Project Context: Frame Divergence Analyzer

## 1. The Goal
To build a lightweight, real-time tool that quantifies "Framing Bias" between news sources.
* **Input:** Two URLs (e.g., CNN vs. Fox News on the same topic).
* **Process:** Scrape text -> Classify into 15 Political Frames -> Calculate "Frame Delta."
* **Output:** "Source A focuses on *Morality* (60%), Source B focuses on *Economics* (40%)."

## 2. The Architecture
* **Model:** `distilbert-base-uncased` (Fine-tuned for Multi-Label Classification).
* **Task:** Multi-Label Text Classification (15 specific political frames).
* **Hardware:** Single GPU (16GB VRAM).

## 3. The Dataset
* **Source:** `copenlu/mm-framing` (Hugging Face).
* **Method:** We use the *text* portion of this dataset.
* **Labels:** The 15 standard Media Frames Corpus labels (Economic, Capacity, Morality, etc.).
* **Note:** The dataset labels were generated by Mistral-7B; we are "distilling" them into DistilBERT.

## Dataset variables

Column name descriptions:
uuid - Unique ID for each article
title - Title of the article
date_publish - Publication date
source_domain - Domain of the publisher
url - Article URL
political_leaning - Political leaning of the publisher

---------- Annotations ---------------

text-topic - Article topic generated from article text
text-topic-exp - Article topic explanation
text-entity-name - Main entity in article text
text-entity-sentiment - Sentiment towards main entity
text-entity-sentiment-exp - Explanation of text sentiment
text-generic-frame - Generic Frame used in Article text
text-generic-frame-exp - Generic Frame in text explanation
text-issue-frame - Issue Frame used in article text
text-issue-frame-exp - Issue Frame explanation
img-generic-frame - Generic Frame used in Article Image
img-frame-exp - Generic Frame in image explanation
img-entity-name - Main subject in Article Image
img-entity-sentiment - Sentiment towards the subject in Article image
img-entity-sentiment-exp - Explanation of image sentiment
gpt-topic - Consolidated topic

## Down-stream evaluation

After framing prediction model is completed, then we might evaluate it on a key real world benchmark- the 

## How was the data above collected? Is it good data?

Authors used Mistral-7B-Instruct-v0.3 available via huggingface 5
with vLLM for high-throughput and memory-efficient
inference and set the parameters as temperature=0.2, max_tokens=4000 dtype=’half’ and max_model_len=8096. In essence they used prompt engineering to instruct the model to read the text and fit classifications into set frames. The original model performance is as follows, testing against the gold-standard media frames corpus.

| Label        | Precision | Recall | F1-score |
| ------------ | --------: | -----: | -------: |
| cap&res      |      0.39 |   0.34 |     0.36 |
| crime        |      0.50 |   0.87 |     0.63 |
| culture      |      0.38 |   0.37 |     0.37 |
| economic     |      0.43 |   0.69 |     0.53 |
| fairness     |      0.17 |   0.74 |     0.28 |
| health       |      0.48 |   0.48 |     0.48 |
| legality     |      0.53 |   0.87 |     0.66 |
| morality     |      0.30 |   0.63 |     0.41 |
| policy       |      0.40 |   0.73 |     0.51 |
| political    |      0.68 |   0.53 |     0.60 |
| public_op    |      0.32 |   0.55 |     0.40 |
| quality_life |      0.28 |   0.36 |     0.31 |
| regulation   |      0.26 |   0.48 |     0.34 |
| security     |      0.30 |   0.45 |     0.36 |
| micro avg    |      0.42 |   0.62 |     0.50 |
| macro avg    |      0.39 |   0.58 |     0.45 |
| weighted avg |      0.45 |   0.62 |     0.51 |
| samples avg  |      0.46 |   0.63 |     0.51 |

## Dataset Quality Analysis

**The Ceiling Problem:** Our distilled models achieve ~0.73 Micro F1, but this is measured against Mistral's labels, not the gold standard. Since Mistral itself only achieves 0.50 F1 against the Media Frames Corpus, we're successfully replicating noisy behavior.

**Why Longformer matches RoBERTa:** Both architectures hit the same ceiling - replicating inconsistent labels. Additional context doesn't help when the labels themselves are unreliable.

**Why Run 3 (Politics-only) worked:** Within a single domain, Mistral likely applied frames more consistently. The model could learn a cleaner signal even if not gold-standard accurate.

**Per-Class Reliability (based on Mistral's gold-standard performance):**
- **Reliable:** Legality (0.66), Crime (0.63), Political (0.60)
- **Moderate:** Economic (0.53), Policy (0.51), Health (0.48)
- **Unreliable:** Fairness (0.28), Quality of Life (0.31), Regulation (0.34), Security (0.36), Cap & Res (0.36), Culture (0.37), Morality (0.41), Public Opinion (0.40)

**Implication for Downstream Use:** For comparing *relative* framing differences between sources (CNN vs Fox), systematic biases may cancel out. Absolute frame detection will be noisy on unreliable classes.


## assistance instructions

Always avoid emojis

I'm working in a conda environment called torch-gpu, when running bash commands, firstly please run:
conda activate torch-gpu

this will activate torch-gpu in your own environment

avoid creating uni-task scripts, including scripts for minute technical tasks. If you do, then please delete them after they have served their purpose.


## Framing Classifier Work

**Objective:** Develop a lightweight multi-label classification model to detect 15 generic media frames (e.g., "Economic," "Fairness," "Legality") for social network analysis.

**Current Architecture:**

* **Base Model:** `roberta-base` (initially experimented with) transitioning to `allenai/longformer-base-4096`.
* **Input Strategy:** Moved from "Head+Tail" truncation (first 320 + last 190 tokens) to full-text input via Longformer to capture mid-document context.
* **Loss Function:** `BCEWithLogitsLoss` utilizing `pos_weight` to penalize missing rare classes (handling 1:50 class imbalances).

**Key Experiments & Findings: from the framing_classier.ipynb**

* **Baseline (Run 1):** `roberta-base` with truncation plateaued at **~0.73 Micro F1**. Post-training threshold optimization (tuning decision boundaries per class) yielded a ~1.1% global performance gain.
* **Weighted Loss (Run 2):** Implementing class weights improved Recall significantly (+1.6%) but hurt Precision. Overall F1 remained tied with the baseline, suggesting an information bottleneck in the input data.
* **Mixture of Experts Hypothesis (Run 3):** A test run training only on "Politics" articles showed massive performance jumps in nuanced frames (**Legality +8%**, **Fairness +8%**). This proved that frame definitions are context-dependent (e.g., "Fairness" means different things in Sports vs. Politics).

**Current Strategy:**

* **Topic Injection:** To operationalize the "Mixture of Experts" gains without maintaining multiple models, we are injecting the `gpt_topic` metadata directly into the input text (e.g., `"[TOPIC: Politics] Article text..."`). This provides the domain signal needed to resolve frame ambiguity.
* **Longformer Implementation:** Adopting Longformer with **Global Attention** set on the `[CLS]` token and injected topic tokens to aggregate context from the entire document.


## Link to a new test set - Sem Eval Task 3
https://propaganda.math.unipd.it/semeval2023task3/teampage.php?passcode=2e9cfe6444bc6c23b3a19209ea58ba6f
